{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERL Complete Training Notebook\n",
    "\n",
    "This notebook provides a complete interface for training language models using **verl** (Volcano Engine Reinforcement Learning).\n",
    "\n",
    "## Supported Algorithms\n",
    "- **GRPO** (Group Relative Policy Optimization)\n",
    "- **PPO** (Proximal Policy Optimization)\n",
    "- **REINFORCE++**\n",
    "- **RLOO** (REINFORCE Leave-One-Out)\n",
    "- **ReMax** (Reward Maximization)\n",
    "\n",
    "## Supported Backends\n",
    "- **vLLM** - Mature, stable, PagedAttention\n",
    "- **SGLang** - Fast, RadixAttention, better for multi-turn\n",
    "\n",
    "## How to Use\n",
    "1. Run Section 0 to install verl (takes ~5-10 minutes)\n",
    "2. Run Section 1 to detect your hardware\n",
    "3. Edit and run Section 1.5 to choose your backend (vLLM or SGLang)\n",
    "4. Edit Section 2 to configure cluster (single GPU / multi GPU / multi node)\n",
    "5. Edit Section 3 to set data paths and model\n",
    "6. Run the algorithm section you want (4-8)\n",
    "7. Monitor training in Section 9\n",
    "8. Upload to HuggingFace in Section 11\n",
    "\n",
    "**Note**: Only run the sections you need. Each algorithm section (4-8) is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 0: Installation\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: This cell takes 5-10 minutes to run. Only run once.\n",
    "\n",
    "Choose your inference backend:\n",
    "- **vLLM**: More mature, stable, good documentation\n",
    "- **SGLang**: Faster, better caching, good for multi-turn\n",
    "- **Both**: Install both to switch easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR INSTALLATION\n",
    "# Uncomment ONE of the following:\n",
    "\n",
    "# Option 1: Install with vLLM\n",
    "# !pip install verl[vllm,gpu,math] jupyter ipywidgets matplotlib tensorboard -q\n",
    "\n",
    "# Option 2: Install with SGLang  \n",
    "# !pip install verl[sglang,gpu,math] jupyter ipywidgets matplotlib tensorboard -q\n",
    "\n",
    "# Option 3: Install both (recommended - can switch easily)\n",
    "!pip install verl[vllm,sglang,gpu,math] jupyter ipywidgets matplotlib tensorboard -q\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Hardware Detection\n",
    "\n",
    "Auto-detect GPUs, CUDA version, bf16 support, and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/user/verl/notebooks')\n",
    "\n",
    "from notebook_utils import (\n",
    "    detect_hardware,\n",
    "    detect_available_backends,\n",
    "    get_recommended_config,\n",
    "    print_hardware_summary,\n",
    "    print_backend_summary,\n",
    "    check_verl_installation,\n",
    "    check_dependencies,\n",
    ")\n",
    "\n",
    "# Check verl installation\n",
    "is_installed, version_or_msg = check_verl_installation()\n",
    "if is_installed:\n",
    "    print(f\"‚úÖ verl {version_or_msg} is installed\\n\")\n",
    "else:\n",
    "    print(f\"‚ùå {version_or_msg}\\n\")\n",
    "    raise ImportError(\"Please install verl first (see Section 0)\")\n",
    "\n",
    "# Detect hardware\n",
    "HARDWARE_INFO = detect_hardware()\n",
    "print_hardware_summary(HARDWARE_INFO)\n",
    "\n",
    "# Detect backends\n",
    "AVAILABLE_BACKENDS = detect_available_backends()\n",
    "print_backend_summary(AVAILABLE_BACKENDS)\n",
    "\n",
    "# Get recommended config based on hardware\n",
    "RECOMMENDED_CONFIG = get_recommended_config(HARDWARE_INFO)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "for key, value in RECOMMENDED_CONFIG.items():\n",
    "    print(f\"{key:35s}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1.5: Backend Selection\n",
    "\n",
    "Choose your inference backend: **vLLM** or **SGLang**\n",
    "\n",
    "### Backend Comparison\n",
    "\n",
    "| Feature | vLLM | SGLang |\n",
    "|---------|------|--------|\n",
    "| Maturity | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê More stable | ‚≠ê‚≠ê‚≠ê‚≠ê Newer |\n",
    "| Speed | ‚≠ê‚≠ê‚≠ê‚≠ê Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Faster |\n",
    "| Multi-turn | ‚≠ê‚≠ê‚≠ê Good | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent |\n",
    "| Caching | PagedAttention | RadixAttention (better) |\n",
    "| Model Support | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Wide | ‚≠ê‚≠ê‚≠ê‚≠ê Growing |\n",
    "\n",
    "**Quick Recommendation**:\n",
    "- Use **vLLM** if you want stability and wide model support\n",
    "- Use **SGLang** if you want maximum speed and better caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import get_backend_config\n",
    "\n",
    "# ===================================================================\n",
    "# CHOOSE YOUR BACKEND\n",
    "# ===================================================================\n",
    "\n",
    "# Uncomment ONE of the following:\n",
    "BACKEND = 'sglang'  # Fast, better caching\n",
    "# BACKEND = 'vllm'   # Stable, mature\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "# Validate backend is installed\n",
    "if not AVAILABLE_BACKENDS.get(BACKEND, False):\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå {BACKEND.upper()} is not installed!\\n\"\n",
    "        f\"Install it with: pip install verl[{BACKEND}]\"\n",
    "    )\n",
    "\n",
    "# Get backend-specific configuration\n",
    "BACKEND_CONFIG = get_backend_config(BACKEND, HARDWARE_INFO)\n",
    "\n",
    "print(f\"‚úÖ Using {BACKEND.upper()} for inference/rollout generation\")\n",
    "print(\"\\nBackend Configuration:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in BACKEND_CONFIG.items():\n",
    "    print(f\"{key:50s}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Cluster Configuration\n",
    "\n",
    "Configure your compute resources:\n",
    "- **Single GPU**: For testing or small models\n",
    "- **Single Node Multi-GPU**: Most common setup (8x A100, etc.)\n",
    "- **Multi-Node Multi-GPU**: For very large models (70B+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import get_cluster_template\n",
    "\n",
    "# ===================================================================\n",
    "# CHOOSE YOUR CLUSTER MODE\n",
    "# ===================================================================\n",
    "\n",
    "# Uncomment ONE of the following:\n",
    "\n",
    "# Option 1: Single GPU (for testing)\n",
    "# CLUSTER_CONFIG = get_cluster_template('single_gpu', HARDWARE_INFO)\n",
    "\n",
    "# Option 2: Single node with multiple GPUs (most common)\n",
    "CLUSTER_CONFIG = get_cluster_template('single_node_multi_gpu', HARDWARE_INFO)\n",
    "\n",
    "# Option 3: Multi-node with multiple GPUs each\n",
    "# CLUSTER_CONFIG = get_cluster_template('multi_node_multi_gpu', HARDWARE_INFO)\n",
    "# IMPORTANT: For multi-node, you MUST set the Ray head node address:\n",
    "# CLUSTER_CONFIG['ray_kwargs.ray_init.address'] = '192.168.1.100:6379'  # Replace with your head node IP\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Cluster Configuration:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CLUSTER_CONFIG.items():\n",
    "    print(f\"{key:50s}: {value}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_gpus = CLUSTER_CONFIG['trainer.n_gpus_per_node'] * CLUSTER_CONFIG['trainer.nnodes']\n",
    "print(f\"\\nüìä Total GPUs to be used: {total_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Data & Model Configuration\n",
    "\n",
    "Set your data paths, model, and training hyperparameters.\n",
    "\n",
    "**Edit the dictionaries below** with your actual paths and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# DATA CONFIGURATION - EDIT THESE PATHS\n",
    "# ===================================================================\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    'train_files': os.path.expanduser('~/data/gsm8k/train.parquet'),\n",
    "    'val_files': os.path.expanduser('~/data/gsm8k/test.parquet'),\n",
    "    'max_prompt_length': 512,\n",
    "    'max_response_length': 1024,\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# MODEL CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'model_path': 'Qwen/Qwen3-8B',  # HuggingFace model or local path\n",
    "    'output_dir': './checkpoints',   # Where to save checkpoints\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# TRAINING HYPERPARAMETERS - EDIT AS NEEDED\n",
    "# ===================================================================\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'learning_rate': 1e-6,\n",
    "    'total_epochs': 15,\n",
    "    'save_freq': 20,      # Save checkpoint every N steps\n",
    "    'test_freq': 5,       # Run validation every N steps\n",
    "    'project_name': 'verl_notebook_training',\n",
    "    'experiment_name': 'gsm8k_qwen3_8b',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "LOGGING_CONFIG = {\n",
    "    'logger': '[\"console\",\"wandb\"]',  # Options: console, wandb, tensorboard, mlflow\n",
    "    'wandb_api_key': None,  # Set this or use 'wandb login' command\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"Training data: {DATA_CONFIG['train_files']}\")\n",
    "print(f\"Validation data: {DATA_CONFIG['val_files']}\")\n",
    "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"Total epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "print(f\"Output directory: {MODEL_CONFIG['output_dir']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ALGORITHM SECTIONS\n",
    "\n",
    "Run **ONLY ONE** of the following algorithm sections (4-8) based on your needs.\n",
    "\n",
    "Each section is self-contained and will:\n",
    "1. Create the configuration\n",
    "2. Initialize Ray cluster\n",
    "3. Start training\n",
    "4. Monitor progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "GRPO is a simplified on-policy algorithm that doesn't require a critic model.\n",
    "\n",
    "**Best for**: Quick experimentation, lower memory usage\n",
    "\n",
    "**Key parameters to edit**:\n",
    "- `actor_rollout_ref.rollout.n`: Number of responses to sample per prompt (default: 5)\n",
    "- `actor_rollout_ref.actor.use_kl_loss`: Whether to use KL divergence loss\n",
    "- `actor_rollout_ref.actor.kl_loss_coef`: KL loss coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import create_config_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ===================================================================\n",
    "# GRPO CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "GRPO_CONFIG = create_config_dict(\n",
    "    algorithm='grpo',\n",
    "    model_path=MODEL_CONFIG['model_path'],\n",
    "    train_files=DATA_CONFIG['train_files'],\n",
    "    val_files=DATA_CONFIG['val_files'],\n",
    "    backend_config=BACKEND_CONFIG,\n",
    "    cluster_config=CLUSTER_CONFIG,\n",
    "    recommended_config=RECOMMENDED_CONFIG,\n",
    "    \n",
    "    # GRPO-specific settings (edit as needed)\n",
    "    **{\n",
    "        'actor_rollout_ref.rollout.n': 5,  # Sample 5 responses per prompt\n",
    "        'actor_rollout_ref.actor.use_kl_loss': True,\n",
    "        'actor_rollout_ref.actor.kl_loss_coef': 0.001,\n",
    "        'actor_rollout_ref.actor.kl_loss_type': 'low_var_kl',\n",
    "        'actor_rollout_ref.actor.entropy_coeff': 0,\n",
    "        'algorithm.use_kl_in_reward': False,\n",
    "        'trainer.critic_warmup': 0,\n",
    "        'trainer.project_name': TRAINING_CONFIG['project_name'],\n",
    "        'trainer.experiment_name': f\"{TRAINING_CONFIG['experiment_name']}_grpo\",\n",
    "        'trainer.total_epochs': TRAINING_CONFIG['total_epochs'],\n",
    "        'trainer.save_freq': TRAINING_CONFIG['save_freq'],\n",
    "        'trainer.test_freq': TRAINING_CONFIG['test_freq'],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"GRPO Configuration created successfully!\")\n",
    "print(f\"Using {BACKEND.upper()} backend with {CLUSTER_CONFIG['trainer.n_gpus_per_node']} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# START GRPO TRAINING\n",
    "# ===================================================================\n",
    "\n",
    "from verl.trainer.main_ppo import run_ppo\n",
    "\n",
    "# Load base config and merge with GRPO config\n",
    "base_config = OmegaConf.load('/home/user/verl/verl/trainer/config/ppo_trainer.yaml')\n",
    "config = OmegaConf.merge(base_config, OmegaConf.create(GRPO_CONFIG))\n",
    "\n",
    "print(\"üöÄ Starting GRPO training...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"   Backend: {BACKEND.upper()}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "print(f\"   Checkpoint dir: {MODEL_CONFIG['output_dir']}\")\n",
    "print(\"\\nTraining will begin in 5 seconds...\\n\")\n",
    "\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Start training\n",
    "run_ppo(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO uses a critic model to estimate value functions.\n",
    "\n",
    "**Best for**: More stable training, better sample efficiency\n",
    "\n",
    "**Key parameters to edit**:\n",
    "- `critic.optim.lr`: Critic learning rate\n",
    "- `critic.model.path`: Critic model (usually same as actor)\n",
    "- `reward_model.enable`: Whether to use a separate reward model\n",
    "- `trainer.critic_warmup`: Number of warmup steps for critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import create_config_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ===================================================================\n",
    "# PPO CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "PPO_CONFIG = create_config_dict(\n",
    "    algorithm='gae',  # GAE (Generalized Advantage Estimation) for PPO\n",
    "    model_path=MODEL_CONFIG['model_path'],\n",
    "    train_files=DATA_CONFIG['train_files'],\n",
    "    val_files=DATA_CONFIG['val_files'],\n",
    "    backend_config=BACKEND_CONFIG,\n",
    "    cluster_config=CLUSTER_CONFIG,\n",
    "    recommended_config=RECOMMENDED_CONFIG,\n",
    "    \n",
    "    # PPO-specific settings (edit as needed)\n",
    "    **{\n",
    "        # Critic configuration\n",
    "        'critic.optim.lr': 1e-5,\n",
    "        'critic.model.path': MODEL_CONFIG['model_path'],  # Same as actor\n",
    "        'critic.model.use_remove_padding': True,\n",
    "        'critic.model.enable_gradient_checkpointing': RECOMMENDED_CONFIG['enable_gradient_checkpointing'],\n",
    "        'critic.ppo_micro_batch_size_per_gpu': RECOMMENDED_CONFIG['ppo_micro_batch_size_per_gpu'],\n",
    "        'critic.model.fsdp_config.param_offload': RECOMMENDED_CONFIG['param_offload'],\n",
    "        'critic.model.fsdp_config.optimizer_offload': RECOMMENDED_CONFIG['optimizer_offload'],\n",
    "        \n",
    "        # Optional: Reward model (set enable=True to use)\n",
    "        'reward_model.enable': False,  # Set to True if you have a reward model\n",
    "        # 'reward_model.model.path': 'path/to/reward/model',  # Uncomment if using reward model\n",
    "        \n",
    "        # Training\n",
    "        'actor_rollout_ref.actor.use_kl_loss': False,\n",
    "        'algorithm.use_kl_in_reward': False,\n",
    "        'trainer.critic_warmup': 0,\n",
    "        'trainer.project_name': TRAINING_CONFIG['project_name'],\n",
    "        'trainer.experiment_name': f\"{TRAINING_CONFIG['experiment_name']}_ppo\",\n",
    "        'trainer.total_epochs': TRAINING_CONFIG['total_epochs'],\n",
    "        'trainer.save_freq': TRAINING_CONFIG['save_freq'],\n",
    "        'trainer.test_freq': TRAINING_CONFIG['test_freq'],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"PPO Configuration created successfully!\")\n",
    "print(f\"Using {BACKEND.upper()} backend with {CLUSTER_CONFIG['trainer.n_gpus_per_node']} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# START PPO TRAINING\n",
    "# ===================================================================\n",
    "\n",
    "from verl.trainer.main_ppo import run_ppo\n",
    "\n",
    "# Load base config and merge with PPO config\n",
    "base_config = OmegaConf.load('/home/user/verl/verl/trainer/config/ppo_trainer.yaml')\n",
    "config = OmegaConf.merge(base_config, OmegaConf.create(PPO_CONFIG))\n",
    "\n",
    "print(\"üöÄ Starting PPO training...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"   Backend: {BACKEND.upper()}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "print(f\"   Checkpoint dir: {MODEL_CONFIG['output_dir']}\")\n",
    "print(\"\\nTraining will begin in 5 seconds...\\n\")\n",
    "\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Start training\n",
    "run_ppo(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: REINFORCE++\n",
    "\n",
    "REINFORCE++ is an improved version of the REINFORCE algorithm with variance reduction.\n",
    "\n",
    "**Best for**: Simpler implementation, good baseline\n",
    "\n",
    "**Key parameters to edit**:\n",
    "- `actor_rollout_ref.rollout.n`: Number of samples per prompt\n",
    "- `algorithm.advantage_normalization`: Whether to normalize advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import create_config_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ===================================================================\n",
    "# REINFORCE++ CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "REINFORCE_PP_CONFIG = create_config_dict(\n",
    "    algorithm='reinforce_plus_plus',\n",
    "    model_path=MODEL_CONFIG['model_path'],\n",
    "    train_files=DATA_CONFIG['train_files'],\n",
    "    val_files=DATA_CONFIG['val_files'],\n",
    "    backend_config=BACKEND_CONFIG,\n",
    "    cluster_config=CLUSTER_CONFIG,\n",
    "    recommended_config=RECOMMENDED_CONFIG,\n",
    "    \n",
    "    # REINFORCE++-specific settings\n",
    "    **{\n",
    "        'actor_rollout_ref.rollout.n': 8,  # Sample 8 responses per prompt\n",
    "        'algorithm.advantage_normalization': True,\n",
    "        'actor_rollout_ref.actor.use_kl_loss': True,\n",
    "        'actor_rollout_ref.actor.kl_loss_coef': 0.001,\n",
    "        'algorithm.use_kl_in_reward': False,\n",
    "        'trainer.critic_warmup': 0,\n",
    "        'trainer.project_name': TRAINING_CONFIG['project_name'],\n",
    "        'trainer.experiment_name': f\"{TRAINING_CONFIG['experiment_name']}_reinforce_pp\",\n",
    "        'trainer.total_epochs': TRAINING_CONFIG['total_epochs'],\n",
    "        'trainer.save_freq': TRAINING_CONFIG['save_freq'],\n",
    "        'trainer.test_freq': TRAINING_CONFIG['test_freq'],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"REINFORCE++ Configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# START REINFORCE++ TRAINING\n",
    "# ===================================================================\n",
    "\n",
    "from verl.trainer.main_ppo import run_ppo\n",
    "\n",
    "base_config = OmegaConf.load('/home/user/verl/verl/trainer/config/ppo_trainer.yaml')\n",
    "config = OmegaConf.merge(base_config, OmegaConf.create(REINFORCE_PP_CONFIG))\n",
    "\n",
    "print(\"üöÄ Starting REINFORCE++ training...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"   Backend: {BACKEND.upper()}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "run_ppo(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: RLOO (REINFORCE Leave-One-Out)\n",
    "\n",
    "RLOO uses leave-one-out baseline for variance reduction.\n",
    "\n",
    "**Best for**: Low variance, good sample efficiency\n",
    "\n",
    "**Key parameters to edit**:\n",
    "- `actor_rollout_ref.rollout.n`: Number of samples (typically higher, e.g., 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import create_config_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ===================================================================\n",
    "# RLOO CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "RLOO_CONFIG = create_config_dict(\n",
    "    algorithm='rloo',\n",
    "    model_path=MODEL_CONFIG['model_path'],\n",
    "    train_files=DATA_CONFIG['train_files'],\n",
    "    val_files=DATA_CONFIG['val_files'],\n",
    "    backend_config=BACKEND_CONFIG,\n",
    "    cluster_config=CLUSTER_CONFIG,\n",
    "    recommended_config=RECOMMENDED_CONFIG,\n",
    "    \n",
    "    # RLOO-specific settings\n",
    "    **{\n",
    "        'actor_rollout_ref.rollout.n': 16,  # Higher sample count for better baseline\n",
    "        'actor_rollout_ref.actor.use_kl_loss': True,\n",
    "        'actor_rollout_ref.actor.kl_loss_coef': 0.001,\n",
    "        'algorithm.use_kl_in_reward': False,\n",
    "        'trainer.critic_warmup': 0,\n",
    "        'trainer.project_name': TRAINING_CONFIG['project_name'],\n",
    "        'trainer.experiment_name': f\"{TRAINING_CONFIG['experiment_name']}_rloo\",\n",
    "        'trainer.total_epochs': TRAINING_CONFIG['total_epochs'],\n",
    "        'trainer.save_freq': TRAINING_CONFIG['save_freq'],\n",
    "        'trainer.test_freq': TRAINING_CONFIG['test_freq'],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"RLOO Configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# START RLOO TRAINING\n",
    "# ===================================================================\n",
    "\n",
    "from verl.trainer.main_ppo import run_ppo\n",
    "\n",
    "base_config = OmegaConf.load('/home/user/verl/verl/trainer/config/ppo_trainer.yaml')\n",
    "config = OmegaConf.merge(base_config, OmegaConf.create(RLOO_CONFIG))\n",
    "\n",
    "print(\"üöÄ Starting RLOO training...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"   Backend: {BACKEND.upper()}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "run_ppo(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: ReMax (Reward Maximization)\n",
    "\n",
    "ReMax focuses on direct reward maximization with sequence balancing.\n",
    "\n",
    "**Best for**: Reward maximization tasks\n",
    "\n",
    "**Key parameters to edit**:\n",
    "- `algorithm.remax_alpha`: Temperature parameter for ReMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import create_config_dict\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# ===================================================================\n",
    "# REMAX CONFIGURATION\n",
    "# ===================================================================\n",
    "\n",
    "REMAX_CONFIG = create_config_dict(\n",
    "    algorithm='remax',\n",
    "    model_path=MODEL_CONFIG['model_path'],\n",
    "    train_files=DATA_CONFIG['train_files'],\n",
    "    val_files=DATA_CONFIG['val_files'],\n",
    "    backend_config=BACKEND_CONFIG,\n",
    "    cluster_config=CLUSTER_CONFIG,\n",
    "    recommended_config=RECOMMENDED_CONFIG,\n",
    "    \n",
    "    # ReMax-specific settings\n",
    "    **{\n",
    "        'actor_rollout_ref.rollout.n': 8,\n",
    "        'algorithm.remax_alpha': 0.01,  # Temperature parameter\n",
    "        'actor_rollout_ref.actor.use_kl_loss': True,\n",
    "        'actor_rollout_ref.actor.kl_loss_coef': 0.001,\n",
    "        'trainer.critic_warmup': 0,\n",
    "        'trainer.project_name': TRAINING_CONFIG['project_name'],\n",
    "        'trainer.experiment_name': f\"{TRAINING_CONFIG['experiment_name']}_remax\",\n",
    "        'trainer.total_epochs': TRAINING_CONFIG['total_epochs'],\n",
    "        'trainer.save_freq': TRAINING_CONFIG['save_freq'],\n",
    "        'trainer.test_freq': TRAINING_CONFIG['test_freq'],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ReMax Configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# START REMAX TRAINING\n",
    "# ===================================================================\n",
    "\n",
    "from verl.trainer.main_ppo import run_ppo\n",
    "\n",
    "base_config = OmegaConf.load('/home/user/verl/verl/trainer/config/ppo_trainer.yaml')\n",
    "config = OmegaConf.merge(base_config, OmegaConf.create(REMAX_CONFIG))\n",
    "\n",
    "print(\"üöÄ Starting ReMax training...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"   Backend: {BACKEND.upper()}\")\n",
    "print(f\"   Epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "run_ppo(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Monitoring & Visualization\n",
    "\n",
    "Monitor training progress and visualize metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensorboard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch tensorboard (change logdir to your checkpoint directory)\n",
    "# %tensorboard --logdir ./checkpoints\n",
    "\n",
    "print(\"TensorBoard loaded. Uncomment the line above to launch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Example: Plot rewards from checkpoint logs\n",
    "# You can customize this based on your logging format\n",
    "\n",
    "def plot_training_metrics(checkpoint_dir):\n",
    "    \"\"\"Plot training metrics from checkpoint directory\"\"\"\n",
    "    # This is a placeholder - adapt based on your actual logging format\n",
    "    print(f\"Looking for metrics in: {checkpoint_dir}\")\n",
    "    \n",
    "    # Example plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].set_title('Average Reward')\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Reward')\n",
    "    \n",
    "    axes[0, 1].set_title('Policy Loss')\n",
    "    axes[0, 1].set_xlabel('Step')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    \n",
    "    axes[1, 0].set_title('KL Divergence')\n",
    "    axes[1, 0].set_xlabel('Step')\n",
    "    axes[1, 0].set_ylabel('KL')\n",
    "    \n",
    "    axes[1, 1].set_title('Learning Rate')\n",
    "    axes[1, 1].set_xlabel('Step')\n",
    "    axes[1, 1].set_ylabel('LR')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to plot\n",
    "# plot_training_metrics(MODEL_CONFIG['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Checkpoint Management\n",
    "\n",
    "List, load, and inspect training checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# List all checkpoints\n",
    "checkpoint_dir = MODEL_CONFIG['output_dir']\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, '*')))\n",
    "    \n",
    "    print(\"Available Checkpoints:\")\n",
    "    print(\"=\"*70)\n",
    "    for i, ckpt in enumerate(checkpoints):\n",
    "        size_mb = sum(os.path.getsize(os.path.join(ckpt, f)) \n",
    "                     for f in os.listdir(ckpt) if os.path.isfile(os.path.join(ckpt, f))) / (1024**2)\n",
    "        print(f\"{i+1}. {os.path.basename(ckpt):30s} ({size_mb:.1f} MB)\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(f\"No checkpoints found in {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a checkpoint for inspection\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Edit this to the checkpoint you want to load\n",
    "CHECKPOINT_TO_LOAD = os.path.join(checkpoint_dir, 'epoch_15')  # Example\n",
    "\n",
    "if os.path.exists(CHECKPOINT_TO_LOAD):\n",
    "    print(f\"Loading checkpoint: {CHECKPOINT_TO_LOAD}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    # model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_TO_LOAD)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_TO_LOAD)\n",
    "    \n",
    "    print(\"‚úÖ Checkpoint loaded successfully!\")\n",
    "    print(\"(Code commented out - uncomment to actually load)\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found: {CHECKPOINT_TO_LOAD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Upload to HuggingFace\n",
    "\n",
    "Upload your trained model to HuggingFace Hub for easy sharing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# HUGGINGFACE UPLOAD CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "HF_CONFIG = {\n",
    "    'checkpoint_path': os.path.join(checkpoint_dir, 'epoch_15'),  # Your trained model\n",
    "    'repo_id': 'your-username/qwen3-8b-gsm8k-grpo',  # EDIT THIS: your HF repo name\n",
    "    'private': False,  # Set to True for private repo\n",
    "    'commit_message': 'Upload trained model',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "# UPLOAD TO HUGGINGFACE\n",
    "# ===================================================================\n",
    "\n",
    "def upload_to_huggingface(config):\n",
    "    \"\"\"Upload model to HuggingFace Hub\"\"\"\n",
    "    \n",
    "    if not os.path.exists(config['checkpoint_path']):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {config['checkpoint_path']}\")\n",
    "    \n",
    "    print(f\"Uploading {config['checkpoint_path']} to {config['repo_id']}...\")\n",
    "    \n",
    "    # Create repository\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        create_repo(\n",
    "            repo_id=config['repo_id'],\n",
    "            private=config['private'],\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        print(f\"‚úÖ Repository created/verified: {config['repo_id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repo: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Upload folder\n",
    "    try:\n",
    "        api.upload_folder(\n",
    "            folder_path=config['checkpoint_path'],\n",
    "            repo_id=config['repo_id'],\n",
    "            repo_type='model',\n",
    "            commit_message=config['commit_message'],\n",
    "        )\n",
    "        print(f\"\\n‚úÖ Model uploaded successfully!\")\n",
    "        print(f\"üîó View at: https://huggingface.co/{config['repo_id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading: {e}\")\n",
    "\n",
    "# Uncomment to upload\n",
    "# upload_to_huggingface(HF_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Cleanup\n",
    "\n",
    "Clean up resources after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Shutdown Ray cluster\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"‚úÖ Ray cluster shutdown\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "print(\"\\nüéâ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Resources\n",
    "\n",
    "- **verl Documentation**: https://verl.readthedocs.io/\n",
    "- **GitHub**: https://github.com/volcengine/verl\n",
    "- **Paper**: [HybridFlow](https://arxiv.org/abs/2409.19256)\n",
    "\n",
    "## Getting Help\n",
    "\n",
    "- Issues: https://github.com/volcengine/verl/issues\n",
    "- Slack: https://join.slack.com/t/verl-project/...\n",
    "- Twitter: @verl_project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
