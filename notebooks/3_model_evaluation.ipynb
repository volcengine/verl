{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERL Model Evaluation Notebook\n",
    "\n",
    "This notebook helps you evaluate trained models from verl.\n",
    "\n",
    "## Features\n",
    "- Load trained checkpoints\n",
    "- Run inference with vLLM or SGLang\n",
    "- Benchmark on test sets\n",
    "- Compare multiple checkpoints\n",
    "- Generate sample outputs\n",
    "- Compute metrics (accuracy, rewards, etc.)\n",
    "\n",
    "## How to Use\n",
    "1. Run installation cell\n",
    "2. Choose your backend (vLLM or SGLang)\n",
    "3. Load your checkpoint\n",
    "4. Run evaluation sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Choose your backend:\n",
    "\n",
    "# Option 1: vLLM\n",
    "# !pip install transformers torch vllm pandas datasets -q\n",
    "\n",
    "# Option 2: SGLang\n",
    "# !pip install transformers torch sglang pandas datasets -q\n",
    "\n",
    "# Option 3: Both\n",
    "!pip install transformers torch vllm sglang pandas datasets -q\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Backend Selection\n",
    "\n",
    "Choose your inference backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# ===================================================================\n",
    "# CHOOSE YOUR BACKEND\n",
    "# ===================================================================\n",
    "\n",
    "# Uncomment ONE:\n",
    "BACKEND = 'vllm'\n",
    "# BACKEND = 'sglang'\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "# Validate\n",
    "available_backends = {\n",
    "    'vllm': importlib.util.find_spec('vllm') is not None,\n",
    "    'sglang': importlib.util.find_spec('sglang') is not None,\n",
    "}\n",
    "\n",
    "if not available_backends[BACKEND]:\n",
    "    raise RuntimeError(f\"‚ùå {BACKEND} not installed! Run: pip install {BACKEND}\")\n",
    "\n",
    "print(f\"‚úÖ Using {BACKEND.upper()} for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Load Checkpoint\n",
    "\n",
    "Load your trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# CHECKPOINT CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "CHECKPOINT_CONFIG = {\n",
    "    'checkpoint_path': './checkpoints/epoch_15',  # Edit this\n",
    "    # OR use HuggingFace model:\n",
    "    # 'checkpoint_path': 'your-username/model-name',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"Loading checkpoint from: {CHECKPOINT_CONFIG['checkpoint_path']}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CHECKPOINT_CONFIG['checkpoint_path'],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# For vLLM/SGLang, we don't load the model here\n",
    "# They handle loading internally for optimal inference\n",
    "print(f\"Model will be loaded by {BACKEND.upper()} backend during inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Initialize Inference Backend\n",
    "\n",
    "Set up vLLM or SGLang for fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# INFERENCE CONFIGURATION - EDIT AS NEEDED\n",
    "# ===================================================================\n",
    "\n",
    "INFERENCE_CONFIG = {\n",
    "    'tensor_parallel_size': 1,  # Set based on your GPU count\n",
    "    'gpu_memory_utilization': 0.8,\n",
    "    'max_model_len': 2048,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.9,\n",
    "    'max_tokens': 1024,\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "if BACKEND == 'vllm':\n",
    "    from vllm import LLM, SamplingParams\n",
    "    \n",
    "    # Initialize vLLM\n",
    "    llm = LLM(\n",
    "        model=CHECKPOINT_CONFIG['checkpoint_path'],\n",
    "        tensor_parallel_size=INFERENCE_CONFIG['tensor_parallel_size'],\n",
    "        gpu_memory_utilization=INFERENCE_CONFIG['gpu_memory_utilization'],\n",
    "        max_model_len=INFERENCE_CONFIG['max_model_len'],\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Sampling params\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=INFERENCE_CONFIG['temperature'],\n",
    "        top_p=INFERENCE_CONFIG['top_p'],\n",
    "        max_tokens=INFERENCE_CONFIG['max_tokens'],\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ vLLM initialized\")\n",
    "\n",
    "elif BACKEND == 'sglang':\n",
    "    import sglang as sgl\n",
    "    \n",
    "    # Initialize SGLang runtime\n",
    "    runtime = sgl.Runtime(\n",
    "        model_path=CHECKPOINT_CONFIG['checkpoint_path'],\n",
    "        tp_size=INFERENCE_CONFIG['tensor_parallel_size'],\n",
    "        mem_fraction_static=INFERENCE_CONFIG['gpu_memory_utilization'],\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    sgl.set_default_backend(runtime)\n",
    "    \n",
    "    print(\"‚úÖ SGLang initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Generate Sample Outputs\n",
    "\n",
    "Generate responses for sample prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SAMPLE PROMPTS - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "SAMPLE_PROMPTS = [\n",
    "    \"What is 25 * 37?\",\n",
    "    \"Solve: If x + 5 = 12, what is x?\",\n",
    "    \"A store has 45 apples and sells 17. How many are left?\",\n",
    "]\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "def generate_responses(prompts):\n",
    "    \"\"\"Generate responses using selected backend\"\"\"\n",
    "    \n",
    "    if BACKEND == 'vllm':\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            results.append({\n",
    "                'prompt': output.prompt,\n",
    "                'response': output.outputs[0].text,\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    elif BACKEND == 'sglang':\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            @sgl.function\n",
    "            def gen(s, prompt):\n",
    "                s += prompt\n",
    "                s += sgl.gen(\n",
    "                    \"response\",\n",
    "                    max_tokens=INFERENCE_CONFIG['max_tokens'],\n",
    "                    temperature=INFERENCE_CONFIG['temperature'],\n",
    "                )\n",
    "            \n",
    "            state = gen.run(prompt=prompt)\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'response': state['response'],\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Generate\n",
    "print(\"Generating responses...\")\n",
    "results = generate_responses(SAMPLE_PROMPTS)\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE GENERATIONS\")\n",
    "print(\"=\"*70)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] Prompt: {result['prompt']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Benchmark on Test Set\n",
    "\n",
    "Evaluate your model on a full test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# TEST SET CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "TEST_CONFIG = {\n",
    "    'test_file': os.path.expanduser('~/data/gsm8k/test.parquet'),\n",
    "    'num_samples': 100,  # Limit for faster testing (set to None for all)\n",
    "    'batch_size': 32,     # For vLLM batch inference\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "# Load test data\n",
    "print(f\"Loading test data from: {TEST_CONFIG['test_file']}\")\n",
    "test_df = pd.read_parquet(TEST_CONFIG['test_file'])\n",
    "\n",
    "if TEST_CONFIG['num_samples']:\n",
    "    test_df = test_df.head(TEST_CONFIG['num_samples'])\n",
    "\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Extract prompts\n",
    "prompts = test_df['prompt'].tolist()\n",
    "\n",
    "# Generate responses\n",
    "print(\"\\nGenerating responses for test set...\")\n",
    "\n",
    "if BACKEND == 'vllm':\n",
    "    # vLLM can handle batch inference efficiently\n",
    "    all_outputs = []\n",
    "    for i in tqdm(range(0, len(prompts), TEST_CONFIG['batch_size'])):\n",
    "        batch = prompts[i:i + TEST_CONFIG['batch_size']]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "        all_outputs.extend([out.outputs[0].text for out in outputs])\n",
    "    \n",
    "elif BACKEND == 'sglang':\n",
    "    # SGLang sequential generation\n",
    "    all_outputs = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        @sgl.function\n",
    "        def gen(s, prompt):\n",
    "            s += prompt\n",
    "            s += sgl.gen(\"response\", max_tokens=INFERENCE_CONFIG['max_tokens'])\n",
    "        \n",
    "        state = gen.run(prompt=prompt)\n",
    "        all_outputs.append(state['response'])\n",
    "\n",
    "# Add to dataframe\n",
    "test_df['model_output'] = all_outputs\n",
    "\n",
    "print(f\"‚úÖ Generated {len(all_outputs)} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Compute Metrics\n",
    "\n",
    "Evaluate model performance with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accuracy for math problems\n",
    "# This is dataset-specific - adjust based on your task\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"Extract numerical answer from text\"\"\"\n",
    "    # Simple extraction - customize based on your format\n",
    "    import re\n",
    "    \n",
    "    # Look for numbers at the end\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return None\n",
    "\n",
    "def compute_accuracy(df):\n",
    "    \"\"\"Compute accuracy for GSM8K-style datasets\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if 'extra_info' in row and 'answer' in row['extra_info']:\n",
    "            ground_truth = row['extra_info']['answer']\n",
    "            predicted = row['model_output']\n",
    "            \n",
    "            # Extract numerical answers\n",
    "            gt_num = extract_answer(str(ground_truth))\n",
    "            pred_num = extract_answer(str(predicted))\n",
    "            \n",
    "            if gt_num and pred_num and gt_num == pred_num:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# Compute metrics\n",
    "if 'extra_info' in test_df.columns:\n",
    "    accuracy, correct, total = compute_accuracy(test_df)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No ground truth available for accuracy computation\")\n",
    "\n",
    "# Show sample correct/incorrect predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "print(test_df[['prompt', 'model_output']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Compare Multiple Checkpoints\n",
    "\n",
    "Compare performance across different training checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CHECKPOINT COMPARISON - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "CHECKPOINTS_TO_COMPARE = [\n",
    "    './checkpoints/epoch_5',\n",
    "    './checkpoints/epoch_10',\n",
    "    './checkpoints/epoch_15',\n",
    "]\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "# This is a template - you would need to reload models and re-run inference\n",
    "# for each checkpoint, which can be time-consuming\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for ckpt_path in CHECKPOINTS_TO_COMPARE:\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f\"‚ö†Ô∏è  Skipping {ckpt_path} (not found)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating checkpoint: {ckpt_path}\")\n",
    "    \n",
    "    # Here you would:\n",
    "    # 1. Reload the model from ckpt_path\n",
    "    # 2. Re-run inference\n",
    "    # 3. Compute metrics\n",
    "    # 4. Store results\n",
    "    \n",
    "    # Placeholder\n",
    "    comparison_results.append({\n",
    "        'checkpoint': os.path.basename(ckpt_path),\n",
    "        'accuracy': 0.0,  # Replace with actual metric\n",
    "    })\n",
    "\n",
    "# Display comparison\n",
    "if comparison_results:\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHECKPOINT COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(comparison_df)\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No checkpoints to compare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Save Results\n",
    "\n",
    "Save evaluation results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SAVE CONFIGURATION - EDIT OUTPUT PATH\n",
    "# ===================================================================\n",
    "\n",
    "SAVE_CONFIG = {\n",
    "    'output_dir': './evaluation_results',\n",
    "    'experiment_name': 'gsm8k_epoch15',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(SAVE_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "output_path = os.path.join(\n",
    "    SAVE_CONFIG['output_dir'],\n",
    "    f\"{SAVE_CONFIG['experiment_name']}_results.parquet\"\n",
    ")\n",
    "\n",
    "test_df.to_parquet(output_path, index=False)\n",
    "print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "\n",
    "# Save metrics summary\n",
    "if 'accuracy' in locals():\n",
    "    metrics_path = os.path.join(\n",
    "        SAVE_CONFIG['output_dir'],\n",
    "        f\"{SAVE_CONFIG['experiment_name']}_metrics.txt\"\n",
    "    )\n",
    "    \n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(f\"Checkpoint: {CHECKPOINT_CONFIG['checkpoint_path']}\\n\")\n",
    "        f.write(f\"Test samples: {len(test_df)}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "        f.write(f\"Correct: {correct}/{total}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Cleanup\n",
    "\n",
    "Clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Shutdown backend\n",
    "if BACKEND == 'sglang':\n",
    "    runtime.shutdown()\n",
    "    print(\"‚úÖ SGLang runtime shutdown\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "print(\"\\nüéâ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've evaluated your trained model!\n",
    "\n",
    "**What you did**:\n",
    "- Loaded a checkpoint\n",
    "- Generated sample outputs\n",
    "- Benchmarked on test set\n",
    "- Computed accuracy metrics\n",
    "- Saved results\n",
    "\n",
    "**Next steps**:\n",
    "- Fine-tune hyperparameters based on results\n",
    "- Compare with baseline models\n",
    "- Upload best checkpoint to HuggingFace (see notebook 1, Section 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
