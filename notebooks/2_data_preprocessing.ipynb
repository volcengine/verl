{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERL Data Preprocessing Notebook\n",
    "\n",
    "This notebook helps you prepare datasets for RL training with verl.\n",
    "\n",
    "## Supported Datasets\n",
    "- **GSM8K** - Grade school math problems\n",
    "- **MATH** - Competition-level math problems\n",
    "- **HH-RLHF** - Helpful and Harmless dialogue\n",
    "- **Custom datasets** - Your own data\n",
    "\n",
    "## Output Format\n",
    "All datasets are converted to **Parquet** format with required columns:\n",
    "- `data_source`: Dataset name\n",
    "- `prompt`: Input prompt\n",
    "- `ability`: Task category (optional)\n",
    "- Other metadata fields\n",
    "\n",
    "## How to Use\n",
    "1. Run the installation cell\n",
    "2. Choose and run the section for your dataset\n",
    "3. The processed data will be saved to the specified output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets pandas pyarrow huggingface_hub -q\n",
    "\n",
    "print(\"✅ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: GSM8K Dataset\n",
    "\n",
    "Grade School Math 8K - Math word problems for elementary school students.\n",
    "\n",
    "**Source**: HuggingFace `openai/gsm8k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# GSM8K CONFIGURATION - EDIT OUTPUT PATH\n",
    "# ===================================================================\n",
    "\n",
    "GSM8K_CONFIG = {\n",
    "    'output_dir': os.path.expanduser('~/data/gsm8k'),  # Edit this\n",
    "    'dataset_name': 'openai/gsm8k',\n",
    "    'subset': 'main',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(GSM8K_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading GSM8K dataset from HuggingFace...\")\n",
    "dataset = load_dataset(GSM8K_CONFIG['dataset_name'], GSM8K_CONFIG['subset'])\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Process train split\n",
    "def process_gsm8k(examples):\n",
    "    \"\"\"Process GSM8K examples to verl format\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for question, answer in zip(examples['question'], examples['answer']):\n",
    "        processed.append({\n",
    "            'data_source': 'gsm8k',\n",
    "            'prompt': question,\n",
    "            'ability': 'math',\n",
    "            'reward_model': {\n",
    "                'style': 'rule',\n",
    "                'ground_truth': answer,\n",
    "            },\n",
    "            'extra_info': {\n",
    "                'answer': answer,\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Process and save train split\n",
    "train_data = process_gsm8k(dataset['train'])\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_path = os.path.join(GSM8K_CONFIG['output_dir'], 'train.parquet')\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "print(f\"✅ Train data saved to: {train_path}\")\n",
    "\n",
    "# Process and save test split\n",
    "test_data = process_gsm8k(dataset['test'])\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_path = os.path.join(GSM8K_CONFIG['output_dir'], 'test.parquet')\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "print(f\"✅ Test data saved to: {test_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: MATH Dataset\n",
    "\n",
    "Competition-level mathematics problems.\n",
    "\n",
    "**Source**: HuggingFace `lighteval/MATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# MATH DATASET CONFIGURATION - EDIT OUTPUT PATH\n",
    "# ===================================================================\n",
    "\n",
    "MATH_CONFIG = {\n",
    "    'output_dir': os.path.expanduser('~/data/math'),  # Edit this\n",
    "    'dataset_name': 'lighteval/MATH',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(MATH_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading MATH dataset from HuggingFace...\")\n",
    "dataset = load_dataset(MATH_CONFIG['dataset_name'])\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Process function\n",
    "def process_math(examples):\n",
    "    \"\"\"Process MATH dataset to verl format\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for i in range(len(examples['problem'])):\n",
    "        processed.append({\n",
    "            'data_source': 'math',\n",
    "            'prompt': examples['problem'][i],\n",
    "            'ability': 'math',\n",
    "            'reward_model': {\n",
    "                'style': 'rule',\n",
    "                'ground_truth': examples['solution'][i],\n",
    "            },\n",
    "            'extra_info': {\n",
    "                'level': examples.get('level', ['unknown'])[i],\n",
    "                'type': examples.get('type', ['unknown'])[i],\n",
    "                'solution': examples['solution'][i],\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Process and save train split\n",
    "train_data = process_math(dataset['train'])\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_path = os.path.join(MATH_CONFIG['output_dir'], 'train.parquet')\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "print(f\"✅ Train data saved to: {train_path}\")\n",
    "\n",
    "# Process and save test split\n",
    "test_data = process_math(dataset['test'])\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_path = os.path.join(MATH_CONFIG['output_dir'], 'test.parquet')\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "print(f\"✅ Test data saved to: {test_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: HH-RLHF Dataset\n",
    "\n",
    "Helpful and Harmless dialogue dataset for RLHF.\n",
    "\n",
    "**Source**: HuggingFace `Anthropic/hh-rlhf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# HH-RLHF CONFIGURATION - EDIT OUTPUT PATH\n",
    "# ===================================================================\n",
    "\n",
    "HH_RLHF_CONFIG = {\n",
    "    'output_dir': os.path.expanduser('~/data/hh_rlhf'),  # Edit this\n",
    "    'dataset_name': 'Anthropic/hh-rlhf',\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(HH_RLHF_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading HH-RLHF dataset from HuggingFace...\")\n",
    "dataset = load_dataset(HH_RLHF_CONFIG['dataset_name'])\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Process function\n",
    "def extract_prompt(conversation):\n",
    "    \"\"\"Extract the user prompt from conversation\"\"\"\n",
    "    # HH-RLHF format: \"\\n\\nHuman: {prompt}\\n\\nAssistant:\"\n",
    "    if '\\n\\nHuman:' in conversation:\n",
    "        parts = conversation.split('\\n\\nHuman:')\n",
    "        if len(parts) > 1:\n",
    "            human_part = parts[1].split('\\n\\nAssistant:')[0].strip()\n",
    "            return human_part\n",
    "    return conversation\n",
    "\n",
    "def process_hh_rlhf(examples):\n",
    "    \"\"\"Process HH-RLHF to verl format\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for chosen in examples['chosen']:\n",
    "        prompt = extract_prompt(chosen)\n",
    "        \n",
    "        processed.append({\n",
    "            'data_source': 'hh_rlhf',\n",
    "            'prompt': prompt,\n",
    "            'ability': 'conversation',\n",
    "            'extra_info': {\n",
    "                'full_conversation': chosen,\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Process and save train split\n",
    "train_data = process_hh_rlhf(dataset['train'])\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_path = os.path.join(HH_RLHF_CONFIG['output_dir'], 'train.parquet')\n",
    "train_df.to_parquet(train_path, index=False)\n",
    "print(f\"✅ Train data saved to: {train_path}\")\n",
    "\n",
    "# Process and save test split\n",
    "test_data = process_hh_rlhf(dataset['test'])\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_path = os.path.join(HH_RLHF_CONFIG['output_dir'], 'test.parquet')\n",
    "test_df.to_parquet(test_path, index=False)\n",
    "print(f\"✅ Test data saved to: {test_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Custom Dataset\n",
    "\n",
    "Process your own custom dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Your data should have at minimum a `prompt` field\n",
    "- Can be CSV, JSON, or any format pandas can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# CUSTOM DATASET CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "CUSTOM_CONFIG = {\n",
    "    'input_file': '/path/to/your/data.csv',  # Edit this\n",
    "    'output_dir': os.path.expanduser('~/data/custom'),  # Edit this\n",
    "    'data_source_name': 'my_custom_dataset',  # Edit this\n",
    "    'prompt_column': 'prompt',  # Column name containing prompts\n",
    "    'ability': 'custom',  # Task category\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(CUSTOM_CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Load your data (adjust based on your file format)\n",
    "# For CSV:\n",
    "# df = pd.read_csv(CUSTOM_CONFIG['input_file'])\n",
    "\n",
    "# For JSON:\n",
    "# df = pd.read_json(CUSTOM_CONFIG['input_file'])\n",
    "\n",
    "# For JSONL:\n",
    "# df = pd.read_json(CUSTOM_CONFIG['input_file'], lines=True)\n",
    "\n",
    "# Example placeholder\n",
    "df = pd.DataFrame({\n",
    "    'prompt': ['Example prompt 1', 'Example prompt 2'],\n",
    "    'other_field': ['value1', 'value2'],\n",
    "})\n",
    "\n",
    "# Convert to verl format\n",
    "def process_custom(row):\n",
    "    \"\"\"Convert custom data to verl format\"\"\"\n",
    "    return {\n",
    "        'data_source': CUSTOM_CONFIG['data_source_name'],\n",
    "        'prompt': row[CUSTOM_CONFIG['prompt_column']],\n",
    "        'ability': CUSTOM_CONFIG['ability'],\n",
    "        'extra_info': {\n",
    "            # Add any other fields you want to preserve\n",
    "            k: v for k, v in row.items() \n",
    "            if k != CUSTOM_CONFIG['prompt_column']\n",
    "        }\n",
    "    }\n",
    "\n",
    "processed_data = [process_custom(row) for _, row in df.iterrows()]\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save\n",
    "output_path = os.path.join(CUSTOM_CONFIG['output_dir'], 'data.parquet')\n",
    "processed_df.to_parquet(output_path, index=False)\n",
    "print(f\"✅ Custom data saved to: {output_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample processed data:\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Data Validation\n",
    "\n",
    "Validate your processed data before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===================================================================\n",
    "# VALIDATION CONFIGURATION - EDIT PATH\n",
    "# ===================================================================\n",
    "\n",
    "DATA_TO_VALIDATE = os.path.expanduser('~/data/gsm8k/train.parquet')  # Edit this\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "def validate_verl_data(file_path):\n",
    "    \"\"\"Validate verl data format\"\"\"\n",
    "    print(f\"Validating: {file_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['data_source', 'prompt']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"✅ All required columns present\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Prompt length statistics\n",
    "    if 'prompt' in df.columns:\n",
    "        prompt_lengths = df['prompt'].str.len()\n",
    "        print(f\"\\nPrompt length statistics:\")\n",
    "        print(f\"  Min: {prompt_lengths.min()}\")\n",
    "        print(f\"  Max: {prompt_lengths.max()}\")\n",
    "        print(f\"  Mean: {prompt_lengths.mean():.2f}\")\n",
    "        print(f\"  Median: {prompt_lengths.median():.2f}\")\n",
    "    \n",
    "    # Data source distribution\n",
    "    if 'data_source' in df.columns:\n",
    "        print(f\"\\nData source distribution:\")\n",
    "        print(df['data_source'].value_counts())\n",
    "    \n",
    "    # Ability distribution\n",
    "    if 'ability' in df.columns:\n",
    "        print(f\"\\nAbility distribution:\")\n",
    "        print(df['ability'].value_counts())\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample records:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"✅ Validation complete!\")\n",
    "\n",
    "# Run validation\n",
    "if os.path.exists(DATA_TO_VALIDATE):\n",
    "    validate_verl_data(DATA_TO_VALIDATE)\n",
    "else:\n",
    "    print(f\"❌ File not found: {DATA_TO_VALIDATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Merge Multiple Datasets\n",
    "\n",
    "Combine multiple datasets for multi-task training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===================================================================\n",
    "# MERGE CONFIGURATION - EDIT THESE\n",
    "# ===================================================================\n",
    "\n",
    "MERGE_CONFIG = {\n",
    "    'datasets_to_merge': [\n",
    "        os.path.expanduser('~/data/gsm8k/train.parquet'),\n",
    "        os.path.expanduser('~/data/math/train.parquet'),\n",
    "    ],\n",
    "    'output_path': os.path.expanduser('~/data/merged/train.parquet'),\n",
    "}\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "os.makedirs(os.path.dirname(MERGE_CONFIG['output_path']), exist_ok=True)\n",
    "\n",
    "# Load and merge\n",
    "dfs = []\n",
    "for dataset_path in MERGE_CONFIG['datasets_to_merge']:\n",
    "    if os.path.exists(dataset_path):\n",
    "        df = pd.read_parquet(dataset_path)\n",
    "        dfs.append(df)\n",
    "        print(f\"✅ Loaded {len(df)} samples from {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️  File not found: {dataset_path}\")\n",
    "\n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle\n",
    "    merged_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Save\n",
    "    merged_df.to_parquet(MERGE_CONFIG['output_path'], index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Merged {len(merged_df)} total samples\")\n",
    "    print(f\"✅ Saved to: {MERGE_CONFIG['output_path']}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"\\nData source distribution:\")\n",
    "    print(merged_df['data_source'].value_counts())\n",
    "else:\n",
    "    print(\"❌ No datasets to merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've now prepared your data for verl training!\n",
    "\n",
    "**Next steps**:\n",
    "1. Go to `1_verl_complete_training.ipynb`\n",
    "2. Set your data paths in Section 3\n",
    "3. Start training with your preferred algorithm\n",
    "\n",
    "**Data format reminder**:\n",
    "- All data is in Parquet format\n",
    "- Required columns: `data_source`, `prompt`\n",
    "- Optional: `ability`, `reward_model`, `extra_info`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
