# Copyright 2025 Meituan Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import asyncio
import time
from pprint import pprint

import ray
from omegaconf import OmegaConf

from recipe.fully_async_policy.message_queue import MessageQueueClient, QueueSample
from verl.single_controller.ray import RayClassWithInitArgs, RayWorkerGroup
from verl.trainer.ppo.ray_trainer import RayPPOTrainer, ResourcePoolManager, Role, WorkerType
from verl.utils.tracking import ValidationGenerationsLogger


@ray.remote(num_cpus=10, max_concurrency=10)
class FullyAsyncRollouter(RayPPOTrainer):
    """
    Asynchronous sample generator, responsible for continuously generating training samples
    and putting them into MessageQueue
    Based on the mature implementation improvements of OneStepOffRayTrainer
    """

    def __init__(
        self,
        config,
        tokenizer,
        role_worker_mapping: dict[Role, WorkerType],
        resource_pool_manager: ResourcePoolManager,
        ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup,
        processor=None,
        reward_fn=None,
        val_reward_fn=None,
        device_name=None,
        max_queue_size=1000,
    ):
        # Store the tokenizer for text processing
        self.tokenizer = tokenizer
        self.processor = processor
        self.config = config
        self.reward_fn = reward_fn
        self.val_reward_fn = val_reward_fn

        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine
        assert not self.hybrid_engine

        self.role_worker_mapping = role_worker_mapping
        self.resource_pool_manager = resource_pool_manager
        self.ray_worker_group_cls = ray_worker_group_cls
        self.device_name = device_name if device_name else self.config.trainer.device
        self.validation_generations_logger = ValidationGenerationsLogger(
            project_name=self.config.trainer.project_name,
            experiment_name=self.config.trainer.experiment_name,
        )

        self.ref_in_actor = False
        self.kl_ctrl_in_reward = False
        self.use_critic = False
        self.use_reference_policy = False
        self.use_rm = False

        print("[FullyAsyncRollouter] Creating datasets...")
        from verl.trainer.main_ppo import create_rl_dataset, create_rl_sampler
        from verl.utils.dataset.rl_dataset import collate_fn

        train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor)
        val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor)
        train_sampler = create_rl_sampler(config.data, train_dataset)

        self._validate_config()
        print(f"[FullyAsyncRollouter] Rollouter _create_dataloader...\n{train_dataset}\n{val_dataset}")

        assert self.config.data.gen_batch_size == 1, "gen_batch_size must be one"

        self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)

        total_rollout_steps = len(self.train_dataloader) * self.config.trainer.total_epochs

        if self.config.rollout.total_rollout_steps is not None:
            total_rollout_steps = self.config.rollout.total_rollout_steps

        self.total_rollout_steps = total_rollout_steps
        print(f"[FullyAsyncRollouter] Total rollout steps: {self.total_rollout_steps}")

        # Rollouter parameter configuration
        self.message_queue_client = None

        self.current_param_version = 0

        # Freshness control - improved configuration management
        async_config = config.async_training
        self.staleness_threshold = async_config.get("staleness_threshold", 3)

        # Statistics
        self.total_generated_samples = 0
        self.train_step_samples = 0
        self.dropped_stale_samples = 0

        # Calculate the samples needed for a train, used to calculate staleness and interrupt rollout
        n_responses_per_prompt = self.config.actor_rollout_ref.rollout.n
        batch_size = self.config.data.train_batch_size
        required_samples = n_responses_per_prompt * batch_size
        self.max_required_samples = required_samples * (self.staleness_threshold + 1)

        # Worker groups
        self.rollout_wg = None
        self.message_queue_client = None

        # Concurrency control
        self.running = False
        self.paused = False
        # Initialize async locks directly - asyncio.Lock() creation is synchronous
        self.lock = asyncio.Lock()
        self.condition = asyncio.Condition(self.lock)

        # Pause/resume statistics
        self.total_pause_time = 0.0
        self.last_pause_time = None

        # Parameter synchronization related
        self.param_synchronizer = None

        # queue size
        self.max_queue_size = max_queue_size

        self.async_rollout_manager = None

        # æµå¼å¤„ç†ç›¸å…³é…ç½®
        self.max_concurrent_samples = async_config.get("max_concurrent_samples", 512)  # æœ€å¤§å¹¶å‘å¤„ç†æ ·æœ¬æ•°

        # æµå¼å¤„ç†ç»Ÿè®¡
        self.max_processing_time = 0.0  # æœ€é•¿å¤„ç†æ—¶é—´
        self.processed_sample_count = 0  # å·²å¤„ç†çš„æ ·æœ¬è®¡æ•°
        self.active_sample_count = 0  # å½“å‰æ­£åœ¨å¤„ç†çš„æ ·æœ¬æ•°
        self.queue_full_pause_count = 0  # é˜Ÿåˆ—æ»¡å¯¼è‡´çš„æš‚åœæ¬¡æ•°

    async def set_message_queue_client(self, message_queue_client: MessageQueueClient):
        """Set message queue client"""
        async with self.lock:
            self.message_queue_client = message_queue_client

    async def set_parameter_synchronizer(self, param_synchronizer):
        """Set parameter synchronizer"""
        async with self.lock:
            self.param_synchronizer = param_synchronizer

    def get_rollout_wg(self):
        """Get rollout worker group"""
        return self.rollout_wg

    async def update_param_version(self, version: int):
        """Update current parameter version"""
        async with self.lock:
            old_version = self.current_param_version
            self.current_param_version = version
            # every time param change, reset train_step_samples
            self.train_step_samples = 0
            print(f"[FullyAsyncRollouter] Parameter version updated from {old_version} to {version}")

    def _validate_config(self):
        # Validate asynchronous training configuration
        if not hasattr(self.config, "async_training"):
            raise ValueError("[FullyAsyncRollouter] Missing async_training configuration")

    def _create_actor_rollout_classes(self):
        # only create rollout
        for role in [Role.Rollout]:
            resource_pool = self.resource_pool_manager.get_resource_pool(role)
            role_cls = RayClassWithInitArgs(
                cls=self.role_worker_mapping[role],
                config=self.config.actor_rollout_ref,
                role=str(role),
            )
            self.resource_pool_to_cls[resource_pool][str(role)] = role_cls

    def _init_models(self):
        self.rollout_wg = self.all_wg[str(Role.Rollout)]
        self.rollout_wg.init_model()
        self.actor_rollout_wg = self.rollout_wg

    def _create_continuous_iterator(self):
        """
        Create a continuous data iterator across epoch
        """
        for epoch in range(self.config.rollout.total_epochs):
            iterator = iter(self.train_dataloader)
            for batch_dict in iterator:
                yield epoch, batch_dict

    def _init_async_rollout_manager(self):
        # create async rollout manager and request scheduler
        assert self.config.actor_rollout_ref.rollout.mode == "async"
        from verl.experimental.agent_loop import AgentLoopManager

        self.async_rollout_mode = True
        self.async_rollout_manager = AgentLoopManager(
            config=self.config,
            worker_group=self.rollout_wg,
        )

    # æ·»åŠ æ ·æœ¬åˆ°å¾…å¤„ç†é˜Ÿåˆ—çš„åç¨‹
    async def _feed_samples(self):
        continuous_iterator = self._create_continuous_iterator()
        sample_count = 0
        for epoch, batch_dict in continuous_iterator:
            # å‡†å¤‡æ ·æœ¬æ•°æ®
            sample_id = f"sample_{epoch}_{sample_count}"
            batch, gen_batch = self._prepare_generate_batch(batch_dict)

            sample_data = {"sample_id": sample_id, "gen_batch": gen_batch, "epoch": epoch, "timestamp": time.time()}

            await self.pending_samples_queue.put(sample_data)
            sample_count += 1

            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æœ€åŽä¸€æ­¥
            if self.global_steps >= self.total_rollout_steps:
                print("[FullyAsyncRollouter] è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œåœæ­¢æ·»åŠ æ–°æ ·æœ¬")
                break

            self.global_steps += 1

        # å‘é€ç»“æŸä¿¡å·
        await self.pending_samples_queue.put("DONE")

    async def _submit_worker(self):
        """æµå¼å¤„ç†å·¥ä½œåç¨‹ - é€ä¸ªæ ·æœ¬ç«‹å³æäº¤å¤„ç†ï¼Œä¸ç­‰å¾…æ‰¹æ¬¡"""
        active_tasks = set()

        while True:
            # èŽ·å–å¾…å¤„ç†æ ·æœ¬
            sample_data = await self.pending_samples_queue.get()

            if sample_data == "DONE":
                print("æ”¶åˆ°ç»“æŸä¿¡å·ï¼Œç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ...")
                # ç­‰å¾…æ‰€æœ‰æ´»åŠ¨ä»»åŠ¡å®Œæˆ
                if active_tasks:
                    await asyncio.gather(*active_tasks, return_exceptions=True)
                break

            # æ£€æŸ¥å¹¶å‘æ•°æ˜¯å¦è¶…é™
            while len(active_tasks) >= self.max_concurrent_samples:
                print(f"è¾¾åˆ°æœ€å¤§å¹¶å‘æ•° {self.max_concurrent_samples}ï¼Œç­‰å¾…ä»»åŠ¡å®Œæˆ...")
                # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                done_tasks, active_tasks = await asyncio.wait(active_tasks, return_when=asyncio.FIRST_COMPLETED)
                # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡
                for task in done_tasks:
                    await task

            # ç«‹å³æäº¤å•ä¸ªæ ·æœ¬å¤„ç†
            task = asyncio.create_task(
                self._process_single_sample_streaming(sample_data), name=f"process_{sample_data['sample_id']}"
            )
            active_tasks.add(task)

            # æ ‡è®°é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
            self.pending_samples_queue.task_done()

    async def _process_single_sample_streaming(self, sample_data: dict):
        """æµå¼å¤„ç†å•ä¸ªæ ·æœ¬"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœå¤„ç†
        if await self._should_pause_generation():
            print(f"[FullyAsyncRollouter] æš‚åœå¤„ç†æ ·æœ¬ {sample_data['sample_id']}")
            # æš‚åœæ—¶é‡æ–°æ”¾å›žé˜Ÿåˆ—
            await self.pending_samples_queue.put(sample_data)
            return

        start_time = time.time()
        # ç›´æŽ¥ä½¿ç”¨AgentLoopManagerçš„å•æ ·æœ¬å¼‚æ­¥å¤„ç†èƒ½åŠ›
        agent_loop_output, processing_time = await self.async_rollout_manager.generate_single_sample_async(
            sample_data["gen_batch"], sample_data["sample_id"]
        )
        end_time = time.time()

        # ç»„è£…æœ€ç»ˆç»“æžœ
        final_result = {
            "sample_id": sample_data["sample_id"],
            "agent_loop_output": agent_loop_output,
            "processing_time": processing_time,
            "timestamp": time.time(),
            "param_version": self.current_param_version,
            "epoch": sample_data["epoch"],
        }

        # ç«‹å³æ”¾å…¥ç»“æžœé˜Ÿåˆ—
        await self.result_queue.put(final_result)

        async with self.lock:
            self.processed_sample_count += 1
            # æ›´æ–°æœ€å¤§å¤„ç†æ—¶é—´ç»Ÿè®¡
            if processing_time > self.max_processing_time:
                self.max_processing_time = processing_time

        print(
            f"[FullyAsyncRollouter] æ ·æœ¬ {sample_data['sample_id']} å¤„ç†å®Œæˆï¼Œ"
            f"è€—æ—¶ {processing_time:.2f}s {end_time - start_time:.2f}s"
        )

    async def _consumer_worker(self):
        """æ¶ˆè´¹è€…åç¨‹ï¼Œè´Ÿè´£ä»Žç»“æžœé˜Ÿåˆ—èŽ·å–å¤„ç†ç»“æžœå¹¶æ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—"""
        while True:
            async with self.lock:
                if not self.running:
                    # å¦‚æžœç³»ç»Ÿåœæ­¢ä½†è¿˜æœ‰ç»“æžœå¾…å¤„ç†ï¼Œç»§ç»­å¤„ç†
                    if self.result_queue.empty():
                        break

            # ä»Žç»“æžœé˜Ÿåˆ—èŽ·å–å¤„ç†ç»“æžœ
            result = await self.result_queue.get()

            # å‡†å¤‡rollout metadata
            rollout_metadata = {
                "generation_timestamp": result["timestamp"],
                "rollout_param_version": result["param_version"],
                "processing_time": result["processing_time"],
                "epoch": result["epoch"],
                "agent_loop_metrics": result["agent_loop_output"].metrics.model_dump(),
            }

            # ç›´æŽ¥å°† AgentLoopOutput æ”¾å…¥æ¶ˆæ¯é˜Ÿåˆ—
            queue_sample = QueueSample(
                data=result["agent_loop_output"],  # ç›´æŽ¥å­˜å‚¨ AgentLoopOutput
                rollout_metadata=rollout_metadata,
            )
            success = self.message_queue_client.put_sample(
                sample=ray.cloudpickle.dumps(queue_sample),
                param_version=result["param_version"],
            )

            async with self.lock:
                if success:
                    self.total_generated_samples += 1
                    self.train_step_samples += 1
                else:
                    self.dropped_stale_samples += 1

            print(
                f"[FullyAsyncRollouter] ðŸ”¥ æ¶ˆè´¹æ ·æœ¬ {result['sample_id']}: "
                f"{'æˆåŠŸ' if success else 'å¤±è´¥'}æ”¾å…¥åˆ°æ¶ˆæ¯é˜Ÿåˆ—, "
                f"å¤„ç†æ—¶é—´ {result['processing_time']:.2f}s"
            )

            # æ ‡è®°ç»“æžœé˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
            self.result_queue.task_done()

    async def _streaming_generation_main(self):
        """æµå¼å¤„ç†çš„ä¸»å…¥å£æ–¹æ³•ï¼ŒåŒ…å«åˆå§‹åŒ–å’ŒéªŒè¯é€»è¾‘"""
        from verl.utils.tracking import Tracking

        self.logger = Tracking(
            project_name=self.config.trainer.project_name,
            experiment_name=self.config.trainer.experiment_name,
            default_backend=self.config.trainer.logger,
            config=OmegaConf.to_container(self.config, resolve=True),
        )

        self.global_steps = 0

        # load checkpoint before doing anything
        self._load_checkpoint()

        # perform validation before training
        # currently, we only support validation using the reward_function.
        if self.val_reward_fn is not None and self.config.trainer.get("val_before_train", True):
            val_metrics = self._validate()
            assert val_metrics, f"{val_metrics=}"
            pprint(f"[FullyAsyncRollouter] Initial validation metrics: {val_metrics}")
            self.logger.log(data=val_metrics, step=self.global_steps)
            if self.config.trainer.get("val_only", False):
                return

        # we start from step 1
        self.global_steps += 1

        # ç¡®ä¿async_rollout_managerå·²ç»åˆå§‹åŒ–
        if self.async_rollout_manager is None:
            self._init_async_rollout_manager()

        # å¯åŠ¨æµå¼å¤„ç†å¾ªçŽ¯
        """æµå¼æ ·æœ¬ç”Ÿæˆä¸»å¾ªçŽ¯ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿å…ˆå®Œæˆçš„æ ·æœ¬ä¼˜å…ˆè¿›å…¥é˜Ÿåˆ—"""
        print(f"[FullyAsyncRollouter] å¯åŠ¨æµå¼å¤„ç†æ¨¡å¼ï¼Œæœ€å¤§å¹¶å‘æ ·æœ¬æ•°: {self.max_concurrent_samples}")

        # åˆå§‹åŒ–å¼‚æ­¥é˜Ÿåˆ—
        self.pending_samples_queue = asyncio.Queue(maxsize=self.max_concurrent_samples)
        self.result_queue = asyncio.Queue()

        # å¯åŠ¨æµå¼å¤„ç†åç¨‹å’Œæ¶ˆè´¹è€…åç¨‹
        self.feed_task = asyncio.create_task(self._feed_samples())
        self.stream_processor_task = asyncio.create_task(self._submit_worker())
        self.consumer_task = asyncio.create_task(self._consumer_worker())
        # å¯åŠ¨æ ·æœ¬æ·»åŠ åç¨‹

        try:
            # ç­‰å¾…æ ·æœ¬æ·»åŠ å®Œæˆ
            await self.feed_task
            print("[FullyAsyncRollouter] æ ·æœ¬æ·»åŠ å®Œæˆ")

            # ç­‰å¾…æµå¼å¤„ç†å®Œæˆ
            await self.stream_processor_task
            print("[FullyAsyncRollouter] æµå¼å¤„ç†å®Œæˆ")

            # ç­‰å¾…ç»“æžœé˜Ÿåˆ—æ¸…ç©º
            await self.result_queue.join()
            print("[FullyAsyncRollouter] æ‰€æœ‰ç»“æžœå¤„ç†å®Œæˆ")

        except Exception as e:
            print(f"[FullyAsyncRollouter] æµå¼å¤„ç†å¼‚å¸¸: {e}")

        finally:
            # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
            if self.stream_processor_task:
                self.stream_processor_task.cancel()
            if self.consumer_task:
                self.consumer_task.cancel()

            # ç­‰å¾…ä»»åŠ¡ç»“æŸ
            await asyncio.gather(self.stream_processor_task, self.consumer_task, return_exceptions=True)

        async with self.lock:
            self.running = False

        # å‘é€ç»ˆæ­¢ä¿¡å·
        self.message_queue_client.put_sample(
            sample=None,
            param_version=self.current_param_version,
        )

    def fit(self):
        """Start the async rollouter - entry point that sets up and runs async tasks"""
        print("[FullyAsyncRollouter] Starting FullyAsyncRollouter...")

        if self.message_queue_client is None:
            raise ValueError("MessageQueue client not set. Call set_message_queue_client() first.")
        if self.param_synchronizer is None:
            raise ValueError("param_synchronizer client not set. Call set_parameter_synchronizer() first.")

        # Run everything in a single async event loop
        asyncio.run(self._async_fit())

    async def _async_fit(self):
        """Main async fit method that coordinates all coroutines"""
        # è®¾ç½®è¿è¡ŒçŠ¶æ€
        async with self.lock:
            self.running = True
            self.paused = False

        # åˆ›å»ºä¸»è¦çš„å¼‚æ­¥ä»»åŠ¡
        generation_task = asyncio.create_task(self._streaming_generation_main())
        monitor_task = asyncio.create_task(self._async_monitor_loop())

        try:
            # å¹¶å‘è¿è¡Œç”Ÿæˆå’Œç›‘æŽ§ä»»åŠ¡
            await asyncio.gather(generation_task, monitor_task, return_exceptions=True)
        except Exception as e:
            print(f"[FullyAsyncRollouter] å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
        finally:
            # æ¸…ç†ä»»åŠ¡
            if not generation_task.done():
                generation_task.cancel()
            if not monitor_task.done():
                monitor_task.cancel()

            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            await asyncio.gather(generation_task, monitor_task, return_exceptions=True)

        print("[FullyAsyncRollouter] Rollouter fit completed")

    async def _async_monitor_loop(self):
        """
        Async coroutine for monitoring:
        Function 1: Log information output
        Function 2: Trigger rollout recovery
        """
        last_stats_time = time.time()
        stats_interval = 30.0
        check_interval = 5.0

        while True:
            async with self.lock:
                if not self.running:
                    break

            await asyncio.sleep(check_interval)

            # å®šæœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯
            current_time = time.time()
            if current_time - last_stats_time >= stats_interval:
                stats = await self.get_statistics()
                print(f"[FullyAsyncRollouter] {stats}")
                last_stats_time = current_time

            if not await self._should_pause_generation():
                await self.resume()

    async def _should_pause_generation(self) -> bool:
        """Determine whether the build should be paused"""
        queue_stats = self.message_queue_client.get_statistics()
        queue_size = queue_stats["queue_size"]
        current_trainer_version = queue_stats["current_param_version"]

        async with self.lock:
            version_diff = self.current_param_version - current_trainer_version

            if version_diff > self.staleness_threshold:
                print(
                    "[FullyAsyncRollouter] "
                    f"Should pause due to version_diff > self.staleness_threshold: "
                    f"rollout_version={self.current_param_version}, "
                    f"trainer_version={current_trainer_version}, diff={version_diff}"
                )
                return True

            if queue_size >= self.max_queue_size:
                print(
                    f"[FullyAsyncRollouter] Should pause due to full queue: "
                    f"size={queue_size}, max={self.max_queue_size}"
                )
                return True

            if self.train_step_samples >= self.max_required_samples:
                print(
                    f"[FullyAsyncRollouter] Should pause due to step_generated_samples >= max_required_samples: "
                    f"self.step_generated_samples={self.train_step_samples}, max={self.max_required_samples}"
                )
                return True

            return False

    async def pause(self) -> bool:
        """pause rollout
        TODO integrated Partial Rollout
        """
        print("[FullyAsyncRollouter] pause")
        async with self.lock:
            if not self.running:
                return False

            if self.paused:
                return True

            self.paused = True
            return True

    async def resume(self) -> bool:
        """resume rollout
        TODO integrated Partial Rollout
        """
        print("[FullyAsyncRollouter] resume")
        async with self.lock:
            if not self.running:
                return False

            if not self.paused:
                return True

            self.paused = False
            self.condition.notify_all()
            return True

    async def get_statistics(self) -> dict:
        async with self.lock:
            queue_stats = self.message_queue_client.get_statistics()
            stats = {
                "is_running": self.running,
                "total_generated_samples": self.total_generated_samples,
                "train_step_samples": self.train_step_samples,
                "dropped_stale_samples": self.dropped_stale_samples,
                "current_param_version": self.current_param_version,
                "queue_size": queue_stats["queue_size"],
                "queue_max_size": self.max_queue_size,
                "max_concurrent_samples": self.max_concurrent_samples,
                "max_processing_time": self.max_processing_time,
                "pending_samples_queue_size": self.pending_samples_queue.qsize(),
                "result_queue_size": self.result_queue.qsize(),
            }

            return stats
