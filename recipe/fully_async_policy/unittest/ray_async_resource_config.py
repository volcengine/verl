# Copyright 2025 Meituan Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import asyncio
import random
import time

import ray


# é…ç½®1: é»˜è®¤é…ç½®
class DefaultStreamingActor:
    """é»˜è®¤é…ç½®çš„æµå¼å¤„ç†Actor"""

    def __init__(self, actor_id: str):
        self.actor_id = actor_id
        self.processed_count = 0
        self.start_time = time.time()
        self.max_concurrent_tasks = 0
        self.current_tasks = 0

    async def process_data_async(self, data_item: dict) -> dict:
        """å¼‚æ­¥å¤„ç†æ•°æ®"""
        self.current_tasks += 1
        self.max_concurrent_tasks = max(self.max_concurrent_tasks, self.current_tasks)

        try:
            task_id = data_item["id"]
            processing_time = random.uniform(1, 3)

            print(f"[{self.actor_id}] å¼€å§‹å¤„ç† {task_id} (å½“å‰å¹¶å‘: {self.current_tasks})")

            # CPUå¯†é›†å‹ä»»åŠ¡æ¨¡æ‹Ÿ
            await asyncio.sleep(processing_time * 0.5)  # I/Oéƒ¨åˆ†

            # æ¨¡æ‹ŸCPUè®¡ç®—
            total = 0
            for i in range(int(processing_time * 100000)):  # CPUå¯†é›†è®¡ç®—
                total += i * 0.001

            await asyncio.sleep(processing_time * 0.5)  # æ›´å¤šI/O

            self.processed_count += 1

            result = {
                "id": task_id,
                "actor_id": self.actor_id,
                "processing_time": processing_time,
                "processed_count": self.processed_count,
                "max_concurrent": self.max_concurrent_tasks,
                "compute_result": total,
                "completed_at": time.time(),
            }

            print(f"[{self.actor_id}] å®Œæˆå¤„ç† {task_id} (è€—æ—¶: {processing_time:.1f}s)")
            return result

        finally:
            self.current_tasks -= 1

    def get_stats(self) -> dict:
        return {
            "actor_id": self.actor_id,
            "processed_count": self.processed_count,
            "max_concurrent_tasks": self.max_concurrent_tasks,
            "uptime": time.time() - self.start_time,
        }


# é…ç½®2: åªè®¾ç½® num_cpus
@ray.remote(num_cpus=4)
class HighCpuStreamingActor(DefaultStreamingActor):
    """é«˜CPUé…ç½®çš„Actor"""

    pass


# é…ç½®3: åªè®¾ç½® max_concurrency
@ray.remote(max_concurrency=5)
class HighConcurrencyStreamingActor(DefaultStreamingActor):
    """é«˜å¹¶å‘é…ç½®çš„Actor"""

    pass


# é…ç½®4: åŒæ—¶è®¾ç½®ä¸¤è€…
@ray.remote(num_cpus=4, max_concurrency=8)
class OptimalStreamingActor(DefaultStreamingActor):
    """æœ€ä¼˜é…ç½®çš„Actor"""

    pass


# é…ç½®5: æç«¯ä½é…ç½®
@ray.remote(num_cpus=1, max_concurrency=2)
class LowResourceStreamingActor(DefaultStreamingActor):
    """ä½èµ„æºé…ç½®çš„Actor"""

    pass


class RayStreamingSystemTest:
    """Rayæµå¼å¤„ç†ç³»ç»Ÿæµ‹è¯•"""

    def __init__(self):
        self.test_data = []
        self.results = {}

    def generate_test_data(self, count: int = 20) -> list[dict]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        return [
            {"id": f"task_{i:03d}", "content": f"æµ‹è¯•æ•°æ®_{i}", "priority": random.choice(["high", "normal", "low"])}
            for i in range(count)
        ]

    async def test_actor_configuration(self, actor_class, config_name: str, test_data: list[dict]) -> dict:
        """æµ‹è¯•ç‰¹å®šé…ç½®çš„Actor"""
        print(f"\n{'=' * 60}")
        print(f"æµ‹è¯•é…ç½®: {config_name}")
        print(f"{'=' * 60}")

        # åˆ›å»ºActorå®ä¾‹
        actor = actor_class.remote(config_name)

        start_time = time.time()

        # å¹¶å‘æäº¤æ‰€æœ‰ä»»åŠ¡
        print(f"æäº¤ {len(test_data)} ä¸ªä»»åŠ¡...")
        task_futures = []

        for i, data_item in enumerate(test_data):
            future = actor.process_data_async.remote(data_item)
            task_futures.append(future)

            # æ¨¡æ‹Ÿæµå¼æ•°æ®åˆ°è¾¾
            if i < len(test_data) - 1:
                await asyncio.sleep(0.1)  # 100msé—´éš”

        print("æ‰€æœ‰ä»»åŠ¡å·²æäº¤ï¼Œç­‰å¾…å®Œæˆ...")

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        try:
            results = await asyncio.gather(*[asyncio.wrap_future(future.future()) for future in task_futures])
        except Exception as e:
            print(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            results = []

        end_time = time.time()
        total_time = end_time - start_time

        # è·å–Actorç»Ÿè®¡ä¿¡æ¯
        stats = ray.get(actor.get_stats.remote())

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {
            "config_name": config_name,
            "total_tasks": len(test_data),
            "completed_tasks": len(results),
            "total_time": total_time,
            "throughput": len(results) / total_time if total_time > 0 else 0,
            "avg_processing_time": sum(r.get("processing_time", 0) for r in results) / len(results) if results else 0,
            "max_concurrent_tasks": stats["max_concurrent_tasks"],
            "actor_stats": stats,
            "success_rate": len(results) / len(test_data) if test_data else 0,
        }

        print(f"âœ… å®Œæˆæµ‹è¯• {config_name}:")
        print(f"   æ€»ä»»åŠ¡æ•°: {performance_metrics['total_tasks']}")
        print(f"   å®Œæˆä»»åŠ¡æ•°: {performance_metrics['completed_tasks']}")
        print(f"   æ€»è€—æ—¶: {performance_metrics['total_time']:.2f}s")
        print(f"   ååé‡: {performance_metrics['throughput']:.2f} tasks/s")
        print(f"   æœ€å¤§å¹¶å‘: {performance_metrics['max_concurrent_tasks']}")
        print(f"   æˆåŠŸç‡: {performance_metrics['success_rate'] * 100:.1f}%")

        return performance_metrics

    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹Rayå¼‚æ­¥èµ„æºé…ç½®æµ‹è¯•")
        print(f"Rayé›†ç¾¤çŠ¶æ€: {ray.cluster_resources()}")

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = self.generate_test_data(15)  # 15ä¸ªä»»åŠ¡ä¾¿äºè§‚å¯Ÿ

        # æµ‹è¯•é…ç½®åˆ—è¡¨
        test_configs = [
            (DefaultStreamingActor, "é»˜è®¤é…ç½® (æ— ç‰¹æ®Šè®¾ç½®)"),
            (HighCpuStreamingActor, "é«˜CPUé…ç½® (num_cpus=4)"),
            (HighConcurrencyStreamingActor, "é«˜å¹¶å‘é…ç½® (max_concurrency=5)"),
            (OptimalStreamingActor, "æœ€ä¼˜é…ç½® (num_cpus=4, max_concurrency=8)"),
            (LowResourceStreamingActor, "ä½èµ„æºé…ç½® (num_cpus=1, max_concurrency=2)"),
        ]

        results = {}

        # é€ä¸ªæµ‹è¯•å„ç§é…ç½®
        for actor_class, config_name in test_configs:
            try:
                result = await self.test_actor_configuration(actor_class, config_name, test_data)
                results[config_name] = result

                # æµ‹è¯•é—´éš”
                await asyncio.sleep(2)

            except Exception as e:
                print(f"âŒ æµ‹è¯• {config_name} å¤±è´¥: {e}")
                results[config_name] = {"error": str(e)}

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(results)

        return results

    def generate_comparison_report(self, results: dict):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{'=' * 80}")
        print("ğŸ“Š é…ç½®å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'=' * 80}")

        # è¡¨å¤´
        print(f"{'é…ç½®åç§°':<25} {'ååé‡':<12} {'æœ€å¤§å¹¶å‘':<10} {'å¹³å‡å¤„ç†æ—¶é—´':<15} {'æˆåŠŸç‡':<10}")
        print("-" * 80)

        # æ•°æ®è¡Œ
        best_throughput = 0
        best_config = ""

        for config_name, result in results.items():
            if "error" in result:
                print(f"{config_name:<25} {'é”™è¯¯':<12} {'':<10} {'':<15} {'':<10}")
                continue

            throughput = result.get("throughput", 0)
            max_concurrent = result.get("max_concurrent_tasks", 0)
            avg_time = result.get("avg_processing_time", 0)
            success_rate = result.get("success_rate", 0)

            print(
                f"{config_name:<25} {throughput:<12.2f} {max_concurrent:<10} "
                f"{avg_time:<15.2f} {success_rate * 100:<10.1f}%"
            )

            if throughput > best_throughput:
                best_throughput = throughput
                best_config = config_name

        print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config} (ååé‡: {best_throughput:.2f} tasks/s)")

        # è¯¦ç»†åˆ†æ
        print("\nğŸ“‹ é…ç½®åˆ†æ:")
        print("1. num_cpus ä½œç”¨:")
        print("   - èµ„æºé¢„ç•™: ç¡®ä¿Actoræœ‰è¶³å¤Ÿè®¡ç®—èµ„æº")
        print("   - èŠ‚ç‚¹é€‰æ‹©: Rayé€‰æ‹©æœ‰è¶³å¤ŸCPUçš„èŠ‚ç‚¹")
        print("   - é¿å…èµ„æºç«äº‰: é˜²æ­¢è¿‡åº¦è°ƒåº¦")

        print("\n2. max_concurrency ä½œç”¨:")
        print("   - å¹¶å‘æ§åˆ¶: é™åˆ¶Actorå†…åŒæ—¶æ‰§è¡Œçš„ä»»åŠ¡æ•°")
        print("   - å†…å­˜ä¿æŠ¤: é˜²æ­¢è¿‡å¤šå¹¶å‘å¯¼è‡´å†…å­˜æº¢å‡º")
        print("   - æ€§èƒ½è°ƒä¼˜: å¹³è¡¡å¹¶å‘åº¦å’Œèµ„æºåˆ©ç”¨ç‡")

        print("\n3. å»ºè®®é…ç½®:")
        print("   - CPUå¯†é›†å‹ä»»åŠ¡: è®¾ç½®è¾ƒé«˜çš„num_cpusï¼Œé€‚ä¸­çš„max_concurrency")
        print("   - I/Oå¯†é›†å‹ä»»åŠ¡: è®¾ç½®è¾ƒä½çš„num_cpusï¼Œè¾ƒé«˜çš„max_concurrency")
        print("   - æ··åˆå‹ä»»åŠ¡: å¹³è¡¡ä¸¤ä¸ªå‚æ•°ï¼Œæ ¹æ®å®é™…æµ‹è¯•è°ƒä¼˜")


async def run_resource_stress_test():
    """è¿è¡Œèµ„æºå‹åŠ›æµ‹è¯•"""
    print(f"\n{'=' * 60}")
    print("ğŸ”¥ èµ„æºå‹åŠ›æµ‹è¯•")
    print(f"{'=' * 60}")

    # åˆ›å»ºå¤šä¸ªä¸åŒé…ç½®çš„Actor
    actors = {
        "é«˜å¹¶å‘ä½CPU": OptimalStreamingActor.remote("stress_test_1"),
        "ä½å¹¶å‘é«˜CPU": ray.remote(num_cpus=8, max_concurrency=2)(DefaultStreamingActor).remote("stress_test_2"),
        "å¹³è¡¡é…ç½®": ray.remote(num_cpus=2, max_concurrency=4)(DefaultStreamingActor).remote("stress_test_3"),
    }

    # å¤§é‡å¹¶å‘ä»»åŠ¡
    heavy_workload = [{"id": f"heavy_{i}", "content": f"é‡è½½ä»»åŠ¡_{i}"} for i in range(50)]

    print("æäº¤å¤§é‡å¹¶å‘ä»»åŠ¡ï¼Œè§‚å¯Ÿèµ„æºä½¿ç”¨...")

    all_futures = []
    for actor_name, actor in actors.items():
        print(f"å‘ {actor_name} æäº¤ä»»åŠ¡...")
        for task in heavy_workload[:15]:  # æ¯ä¸ªActorå¤„ç†15ä¸ªä»»åŠ¡
            future = actor.process_data_async.remote(task)
            all_futures.append((actor_name, future))

    # ç­‰å¾…å®Œæˆå¹¶è®°å½•æ—¶é—´
    start_time = time.time()
    results = []

    for actor_name, future in all_futures:
        try:
            result = await asyncio.wrap_future(future.future())
            results.append((actor_name, result))
        except Exception as e:
            print(f"{actor_name} ä»»åŠ¡å¤±è´¥: {e}")

    end_time = time.time()

    print(f"å‹åŠ›æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}s")
    print(f"å®Œæˆä»»åŠ¡æ•°: {len(results)}")

    # æŒ‰Actoråˆ†ç»„ç»Ÿè®¡
    actor_stats = {}
    for actor_name, result in results:
        if actor_name not in actor_stats:
            actor_stats[actor_name] = []
        actor_stats[actor_name].append(result)

    for actor_name, actor_results in actor_stats.items():
        avg_time = sum(r["processing_time"] for r in actor_results) / len(actor_results)
        print(f"{actor_name}: å®Œæˆ {len(actor_results)} ä¸ªä»»åŠ¡, å¹³å‡è€—æ—¶ {avg_time:.2f}s")


async def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–Ray
    if not ray.is_initialized():
        ray.init(
            num_cpus=16,  # è®¾ç½®è¶³å¤Ÿçš„CPUèµ„æº
            object_store_memory=2000000000,  # 2GB
            ignore_reinit_error=True,
        )

    print("ğŸ¯ Rayå¼‚æ­¥èµ„æºé…ç½®æµ‹è¯•")
    print(f"å¯ç”¨èµ„æº: {ray.cluster_resources()}")

    try:
        # åŸºç¡€é…ç½®æµ‹è¯•
        test_system = RayStreamingSystemTest()
        await test_system.run_comprehensive_test()

        # å‹åŠ›æµ‹è¯•
        await run_resource_stress_test()

        print("\næ‰€æœ‰æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # æ¸…ç†èµ„æº
        ray.shutdown()


if __name__ == "__main__":
    asyncio.run(main())
