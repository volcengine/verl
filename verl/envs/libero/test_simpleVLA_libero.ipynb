{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58736775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/veRL/veRL/cyk/vla-mix/verl\")\n",
    "sys.path.append(\"/file_system/cyk/vla_mix/LIBERO/\")\n",
    "import functools\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "original_torch_load = torch.load\n",
    "\n",
    "\n",
    "# @functools.wraps(original_torch_load)\n",
    "# def new_torch_load(*args, **kwargs):\n",
    "#     if \"weights_only\" not in kwargs:\n",
    "#         kwargs[\"weights_only\"] = False\n",
    "#     return original_torch_load(*args, **kwargs)\n",
    "\n",
    "\n",
    "# torch.load = new_torch_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45b7b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LIBERO constants:\n",
      "  NUM_ACTIONS_CHUNK = 8\n",
      "  ACTION_DIM = 7\n",
      "  PROPRIO_DIM = 8\n",
      "  ACTION_PROPRIO_NORMALIZATION_TYPE = NormalizationType.BOUNDS_Q99\n",
      "If needed, manually set the correct constants in `prismatic/vla/constants.py`!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from verl.models.openvla_oft.configuration_prismatic import OpenVLAConfig\n",
    "from verl.models.openvla_oft.modeling_prismatic import OpenVLAForActionPrediction\n",
    "from verl.models.openvla_oft.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor\n",
    "# from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772c1c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "PrismaticForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "OpenVLAForActionPrediction has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "WARNING:2025-10-13 11:51:36,573:Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.53.2` and `tokenizers==0.21.4`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022f8e8de7ee4cb9a866abbee3d48ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PrismaticForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "OpenVLAForActionPrediction has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "PrismaticForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "OpenVLAForActionPrediction has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenVLAForActionPrediction(\n",
       "  (vision_backbone): PrismaticVisionBackbone(\n",
       "    (featurizer): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (23): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (fused_featurizer): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (23): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (24): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (25): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (26): Block(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn_pool): AttentionPoolLatent(\n",
       "        (q): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (kv): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (projector): PrismaticProjector(\n",
       "    (fc1): Linear(in_features=2176, out_features=8704, bias=True)\n",
       "    (fc2): Linear(in_features=8704, out_features=4096, bias=True)\n",
       "    (fc3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (act_fn1): GELU(approximate='none')\n",
       "    (act_fn2): GELU(approximate='none')\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096, padding_idx=32000)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = \"/file_system/common-models/Haozhan72-kangsheng/Openvla-oft-SFT-libero10-trajall/\"\n",
    "INFER_DEVICE = \"cuda:0\"\n",
    "processor = PrismaticProcessor.from_pretrained(local_path, trust_remote_code=True)\n",
    "tokenizer=processor.tokenizer\n",
    "\n",
    "# override model kwargs\n",
    "actor_model_config = OpenVLAConfig.from_pretrained(local_path, trust_remote_code=True)\n",
    "\n",
    "torch_dtype = torch.float32\n",
    "actor_module = OpenVLAForActionPrediction.from_pretrained(\n",
    "                                        pretrained_model_name_or_path=local_path,\n",
    "                                        torch_dtype=torch_dtype,\n",
    "                                        #attn_implementation=\"flash_attention_2\",\n",
    "                                        config=actor_model_config,              \n",
    "                                        trust_remote_code=True,\n",
    "                                    )\n",
    "actor_module.vision_backbone.set_num_images_in_input(1)\n",
    "dataset_statistics_path = os.path.join(local_path, \"dataset_statistics.json\")\n",
    "if os.path.isfile(dataset_statistics_path):\n",
    "    with open(dataset_statistics_path, \"r\") as f:\n",
    "        norm_stats = json.load(f)\n",
    "    actor_module.norm_stats = norm_stats\n",
    "actor_module = actor_module.to(INFER_DEVICE)\n",
    "actor_module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b35527",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "from verl.envs.libero.libero_env import LiberoEnv\n",
    "batch_size = 16\n",
    "cfg_dict = {\n",
    "        \"task_suite_name\": \"libero_10\",\n",
    "        \"num_envs\": batch_size,\n",
    "        \"group_size\": batch_size,\n",
    "        \"num_group\": 1,\n",
    "        \"seed\": 0,\n",
    "        \"use_fixed_reset_state_ids\": False,\n",
    "        \"ignore_terminations\": False,\n",
    "        \"auto_reset\": True,\n",
    "        \"max_episode_steps\": 512,\n",
    "        \"use_rel_reward\": False,\n",
    "        \"reward_coef\": 1.0,\n",
    "        \"only_eval\": False,\n",
    "        \"use_ordered_reset_state_ids\": False,\n",
    "        \"num_images_in_input\": 2,\n",
    "        \"init_params\": {\n",
    "            \"camera_depths\": False,\n",
    "            \"camera_heights\": 512,\n",
    "            \"camera_widths\": 512,\n",
    "            \"camera_names\": [\"agentview\", \"robot0_eye_in_hand\"],\n",
    "        },\n",
    "        \"controller_configs\": {\n",
    "            \"type\": \"OSC_POSE\",\n",
    "            \"input_max\": 1,\n",
    "            \"input_min\": -1,\n",
    "            \"output_max\": [0.05, 0.05, 0.05, 0.5, 0.5, 0.5],\n",
    "            \"output_min\": [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5],\n",
    "            \"kp\": 150,\n",
    "            \"damping_ratio\": 1,\n",
    "            \"impedance_mode\": \"fixed\",\n",
    "            \"kp_limits\": [0, 300],\n",
    "            \"damping_ratio_limits\": [0, 10],\n",
    "            \"position_limits\": None,\n",
    "            \"orientation_limits\": None,\n",
    "            \"uncouple_pos_ori\": True,\n",
    "            \"control_delta\": True,\n",
    "            \"interpolation\": None,\n",
    "            \"ramp_ratio\": 0.2,\n",
    "        },\n",
    "        \"video_cfg\": {\n",
    "            \"save_video\": True,\n",
    "            \"video_base_dir\": \"/tmp/videos\",\n",
    "        },\n",
    "    }\n",
    "cfg = OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "get task and trial id [ 50 100 150 200 250 300 350 400 450 500] [425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425\n",
      " 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425\n",
      " 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425 425\n",
      " 425 425 425 425 425 425 425 425 425 425] [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8] [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get task and trial id [ 50 100 150 200 250 300 350 400 450 500] [318 255 134 153  20  37   8  87 406 324 456 251 303 485 364 316 271 279\n",
      " 467 138 407 335   1 197 428 277  16 382 364 423  87  44 431  11 270  40\n",
      " 149 240 211 201  14   2  62   4 335 262 323 128 307 382 191 230 498 402\n",
      " 490 189 342 475 325 420 344 352 194 437] [6, 5, 2, 3, 0, 0, 0, 1, 8, 6, 9, 5, 6, 9, 7, 6, 5, 5, 9, 2, 8, 6, 0, 3, 8, 5, 0, 7, 7, 8, 1, 0, 8, 0, 5, 0, 2, 4, 4, 4, 0, 0, 1, 0, 6, 5, 6, 2, 6, 7, 3, 4, 9, 8, 9, 3, 6, 9, 6, 8, 6, 7, 3, 8] [18, 5, 34, 3, 20, 37, 8, 37, 6, 24, 6, 1, 3, 35, 14, 16, 21, 29, 17, 38, 7, 35, 1, 47, 28, 27, 16, 32, 14, 23, 37, 44, 31, 11, 20, 40, 49, 40, 11, 1, 14, 2, 12, 4, 35, 12, 23, 28, 7, 32, 41, 30, 48, 2, 40, 39, 42, 25, 25, 20, 44, 2, 44, 37]\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEOFError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m libero_env.is_start = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# The first call to step with actions=None will reset the environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m obs_venv, _, terminated_venv, truncated_venv, info_venv = \u001b[43mlibero_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitial data:\u001b[39m\u001b[33m\"\u001b[39m, obs_venv)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(\"Initial data:\", init_data)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# task_descriptions = [init_data['task_description']]\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# print(f\"Task description: {task_descriptions}\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#         print(f\"Task completed at step {step}.\")\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#         break\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/veRL/veRL/cyk/vla-mix/verl/verl/envs/libero/libero_env.py:310\u001b[39m, in \u001b[36mLiberoEnv.step\u001b[39m\u001b[34m(self, actions, auto_reset)\u001b[39m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_start, \u001b[33m\"\u001b[39m\u001b[33mActions must be provided after the first reset.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_start:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     obs, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreset_state_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset_state_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_fixed_reset_state_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_start = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    312\u001b[39m     terminations = np.zeros(\u001b[38;5;28mself\u001b[39m.num_envs, dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/veRL/veRL/cyk/vla-mix/verl/verl/envs/libero/libero_env.py:292\u001b[39m, in \u001b[36mLiberoEnv.reset\u001b[39m\u001b[34m(self, env_idx, reset_state_ids, options)\u001b[39m\n\u001b[32m    289\u001b[39m     num_reset_states = \u001b[38;5;28mlen\u001b[39m(env_idx)\n\u001b[32m    290\u001b[39m     reset_state_ids = \u001b[38;5;28mself\u001b[39m._get_random_reset_state_ids(num_reset_states)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreset_state_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m    295\u001b[39m     zero_actions = np.zeros((\u001b[38;5;28mself\u001b[39m.num_envs, \u001b[32m7\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/veRL/veRL/cyk/vla-mix/verl/verl/envs/libero/libero_env.py:272\u001b[39m, in \u001b[36mLiberoEnv._reconfigure\u001b[39m\u001b[34m(self, reset_state_ids, env_idx)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reconfig_env_idx:\n\u001b[32m    271\u001b[39m     env_fn_params = \u001b[38;5;28mself\u001b[39m.get_env_fn_params(reconfig_env_idx)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreconfigure_env_fns\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_fn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconfig_env_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m.env.seed([\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(env_idx))\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m.env.reset(\u001b[38;5;28mid\u001b[39m=env_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/veRL/veRL/cyk/vla-mix/verl/verl/envs/libero/venv.py:162\u001b[39m, in \u001b[36mReconfigureSubprocEnv.reconfigure_env_fns\u001b[39m\u001b[34m(self, env_fns, id)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m._assert_id(\u001b[38;5;28mid\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mid\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreconfigure_env_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/veRL/veRL/cyk/vla-mix/verl/verl/envs/libero/venv.py:145\u001b[39m, in \u001b[36mReconfigureSubprocEnvWorker.reconfigure_env_fn\u001b[39m\u001b[34m(self, env_fn_param)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreconfigure_env_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fn_param):\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m.parent_remote.send([\u001b[33m\"\u001b[39m\u001b[33mreconfigure\u001b[39m\u001b[33m\"\u001b[39m, env_fn_param])\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent_remote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/isaaclab/_isaac_sim/kit/python/lib/python3.11/multiprocessing/connection.py:250\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/isaaclab/_isaac_sim/kit/python/lib/python3.11/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/isaaclab/_isaac_sim/kit/python/lib/python3.11/multiprocessing/connection.py:399\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remaining == size:\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgot end of file during message\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mEOFError\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import torch.cuda.profiler as profiler\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "# batch_size = 8\n",
    "# max_steps = 200\n",
    "max_steps = 512\n",
    "# max_steps = 1024\n",
    "# max_steps = 256\n",
    "action_chunks_len = 8\n",
    "libero_env = LiberoEnv(cfg, rank=0, world_size=1)\n",
    "libero_env.is_start = True\n",
    "# The first call to step with actions=None will reset the environment\n",
    "obs_venv, _, terminated_venv, truncated_venv, info_venv = libero_env.step(actions=None)\n",
    "print(\"Initial data:\", obs_venv)\n",
    "# print(\"Initial data:\", init_data)\n",
    "# task_descriptions = [init_data['task_description']]\n",
    "# print(f\"Task description: {task_descriptions}\")\n",
    "\n",
    "# valid_video[init_data['task_file_name']].extend(init_data['valid_images'])\n",
    "# env_data = copy.deepcopy(init_data)\n",
    "# env_obs = env_data['obs']\n",
    "# for step in trange(max_steps // action_chunks_len):\n",
    "#     # print(\"Step:\", step)\n",
    "#     # prof.step()\n",
    "#     inputs = [_obs_to_input(env_obs)]\n",
    "#     vla_input = process_input(inputs, task_descriptions, config)\n",
    "#     # vla_input.update(meta_info)\n",
    "#     vla_output = _generate_one_step(vla_input)\n",
    "#     actions = vla_output[\"action\"]\n",
    "#     step_data = {\n",
    "#         \"responses\": vla_output[\"responses\"],\n",
    "#         \"input_ids\": vla_output[\"input_ids\"],\n",
    "#         \"attention_mask\": vla_output[\"attention_mask\"],\n",
    "#         \"pixel_values\": vla_output[\"pixel_values\"],\n",
    "#         \"action\": actions,\n",
    "#         \"step\": step\n",
    "#     }\n",
    "#     vla_history.append(step_data)\n",
    "    \n",
    "\n",
    "#     result = libero_env.step(actions[0])\n",
    "#     valid_video[init_data['task_file_name']].extend(result['valid_images'])\n",
    "#     env_obs = result[\"obs\"]\n",
    "\n",
    "#     step += action_chunks_len\n",
    "#     complete = result[\"complete\"]\n",
    "#     if complete:\n",
    "#         print(f\"Task completed at step {step}.\")\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images_and_states': {'full_image': tensor([[[[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 115, 112],\n",
       "            [117, 115, 111],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [118, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [117, 114, 111]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 79,  57,  45],\n",
       "            [ 76,  55,  44],\n",
       "            [ 73,  54,  44],\n",
       "            ...,\n",
       "            [ 83,  57,  45],\n",
       "            [ 84,  58,  46],\n",
       "            [ 85,  59,  47]],\n",
       "  \n",
       "           [[ 77,  56,  46],\n",
       "            [ 76,  55,  46],\n",
       "            [ 74,  54,  45],\n",
       "            ...,\n",
       "            [ 82,  57,  44],\n",
       "            [ 82,  58,  45],\n",
       "            [ 83,  58,  46]],\n",
       "  \n",
       "           [[ 77,  56,  47],\n",
       "            [ 76,  55,  47],\n",
       "            [ 76,  55,  47],\n",
       "            ...,\n",
       "            [ 80,  57,  44],\n",
       "            [ 81,  57,  44],\n",
       "            [ 81,  58,  45]]],\n",
       "  \n",
       "  \n",
       "          [[[110, 109, 105],\n",
       "            [111, 109, 106],\n",
       "            [111, 109, 106],\n",
       "            ...,\n",
       "            [110, 108, 105],\n",
       "            [111, 109, 106],\n",
       "            [110, 108, 104]],\n",
       "  \n",
       "           [[111, 109, 106],\n",
       "            [111, 109, 106],\n",
       "            [111, 109, 106],\n",
       "            ...,\n",
       "            [111, 109, 106],\n",
       "            [110, 108, 105],\n",
       "            [109, 107, 104]],\n",
       "  \n",
       "           [[110, 109, 105],\n",
       "            [110, 109, 105],\n",
       "            [110, 109, 105],\n",
       "            ...,\n",
       "            [102, 101,  99],\n",
       "            [101, 101,  99],\n",
       "            [101, 101,  98]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[139, 108,  67],\n",
       "            [138, 107,  67],\n",
       "            [138, 107,  66],\n",
       "            ...,\n",
       "            [117,  87,  50],\n",
       "            [117,  86,  50],\n",
       "            [117,  86,  50]],\n",
       "  \n",
       "           [[139, 107,  66],\n",
       "            [139, 107,  66],\n",
       "            [138, 106,  65],\n",
       "            ...,\n",
       "            [118,  87,  51],\n",
       "            [118,  87,  51],\n",
       "            [117,  87,  50]],\n",
       "  \n",
       "           [[138, 106,  65],\n",
       "            [138, 106,  64],\n",
       "            [138, 105,  64],\n",
       "            ...,\n",
       "            [117,  86,  50],\n",
       "            [117,  86,  50],\n",
       "            [117,  86,  50]]],\n",
       "  \n",
       "  \n",
       "          [[[  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            ...,\n",
       "            [130, 130, 130],\n",
       "            [135, 135, 135],\n",
       "            [140, 140, 140]],\n",
       "  \n",
       "           [[  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            ...,\n",
       "            [ 81,  81,  81],\n",
       "            [ 89,  89,  89],\n",
       "            [ 96,  96,  96]],\n",
       "  \n",
       "           [[  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            [  0,   0,   0],\n",
       "            ...,\n",
       "            [ 19,  19,  19],\n",
       "            [ 21,  21,  21],\n",
       "            [ 25,  25,  25]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[204, 187, 171],\n",
       "            [199, 182, 166],\n",
       "            [195, 178, 162],\n",
       "            ...,\n",
       "            [200, 183, 162],\n",
       "            [201, 184, 164],\n",
       "            [202, 185, 165]],\n",
       "  \n",
       "           [[201, 184, 168],\n",
       "            [196, 180, 164],\n",
       "            [195, 178, 162],\n",
       "            ...,\n",
       "            [199, 182, 162],\n",
       "            [200, 183, 162],\n",
       "            [202, 185, 164]],\n",
       "  \n",
       "           [[200, 183, 167],\n",
       "            [194, 177, 161],\n",
       "            [194, 177, 161],\n",
       "            ...,\n",
       "            [199, 181, 161],\n",
       "            [199, 182, 162],\n",
       "            [201, 184, 164]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 115, 112],\n",
       "            [117, 115, 111],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [118, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [117, 114, 111]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 79,  57,  45],\n",
       "            [ 76,  55,  44],\n",
       "            [ 73,  54,  44],\n",
       "            ...,\n",
       "            [ 83,  57,  45],\n",
       "            [ 84,  58,  46],\n",
       "            [ 85,  59,  47]],\n",
       "  \n",
       "           [[ 77,  56,  46],\n",
       "            [ 76,  55,  46],\n",
       "            [ 74,  54,  45],\n",
       "            ...,\n",
       "            [ 82,  57,  44],\n",
       "            [ 82,  58,  45],\n",
       "            [ 83,  58,  46]],\n",
       "  \n",
       "           [[ 77,  56,  47],\n",
       "            [ 76,  55,  47],\n",
       "            [ 76,  55,  47],\n",
       "            ...,\n",
       "            [ 80,  57,  44],\n",
       "            [ 81,  57,  44],\n",
       "            [ 81,  58,  45]]],\n",
       "  \n",
       "  \n",
       "          [[[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 115, 112],\n",
       "            [117, 115, 111],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [118, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [117, 114, 111]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 79,  57,  45],\n",
       "            [ 76,  55,  44],\n",
       "            [ 73,  54,  44],\n",
       "            ...,\n",
       "            [ 83,  57,  45],\n",
       "            [ 84,  58,  46],\n",
       "            [ 85,  59,  47]],\n",
       "  \n",
       "           [[ 77,  56,  46],\n",
       "            [ 76,  55,  46],\n",
       "            [ 74,  54,  45],\n",
       "            ...,\n",
       "            [ 82,  57,  44],\n",
       "            [ 82,  58,  45],\n",
       "            [ 83,  58,  46]],\n",
       "  \n",
       "           [[ 77,  56,  47],\n",
       "            [ 76,  55,  47],\n",
       "            [ 76,  55,  47],\n",
       "            ...,\n",
       "            [ 80,  57,  44],\n",
       "            [ 81,  57,  44],\n",
       "            [ 81,  58,  45]]],\n",
       "  \n",
       "  \n",
       "          [[[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 115, 112],\n",
       "            [117, 115, 111],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [117, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [116, 114, 110]],\n",
       "  \n",
       "           [[ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            [ 13,  13,  13],\n",
       "            ...,\n",
       "            [118, 116, 112],\n",
       "            [116, 114, 110],\n",
       "            [117, 114, 111]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 79,  57,  45],\n",
       "            [ 76,  55,  44],\n",
       "            [ 73,  54,  44],\n",
       "            ...,\n",
       "            [ 83,  57,  45],\n",
       "            [ 84,  58,  46],\n",
       "            [ 85,  59,  47]],\n",
       "  \n",
       "           [[ 77,  56,  46],\n",
       "            [ 76,  55,  46],\n",
       "            [ 74,  54,  45],\n",
       "            ...,\n",
       "            [ 82,  57,  44],\n",
       "            [ 82,  58,  45],\n",
       "            [ 83,  58,  46]],\n",
       "  \n",
       "           [[ 77,  56,  47],\n",
       "            [ 76,  55,  47],\n",
       "            [ 76,  55,  47],\n",
       "            ...,\n",
       "            [ 80,  57,  44],\n",
       "            [ 81,  57,  44],\n",
       "            [ 81,  58,  45]]]], dtype=torch.uint8),\n",
       "  'state': tensor([[-5.6177e-02, -4.7192e-03,  6.7928e-01,  3.1398e+00,  3.2948e-03,\n",
       "           -9.0280e-02,  2.0541e-02, -2.0544e-02],\n",
       "          [-3.0054e-01,  2.3009e-03,  1.1768e+00,  3.1403e+00, -1.2676e-03,\n",
       "           -8.7724e-02,  2.0543e-02, -2.0543e-02],\n",
       "          [-2.0961e-01,  5.6452e-03,  1.1744e+00,  3.1410e+00,  1.4104e-03,\n",
       "           -9.1271e-02,  2.0545e-02, -2.0541e-02],\n",
       "          [-2.0333e-01,  7.2501e-03,  1.1772e+00,  3.1407e+00,  2.6978e-03,\n",
       "           -8.6675e-02,  2.0543e-02, -2.0543e-02],\n",
       "          [-5.5079e-02,  1.4590e-02,  6.8229e-01,  3.1400e+00,  4.6525e-03,\n",
       "           -8.7061e-02,  2.0538e-02, -2.0544e-02],\n",
       "          [-6.1874e-02,  1.0370e-02,  6.7378e-01,  3.1409e+00,  2.2990e-03,\n",
       "           -9.1497e-02,  2.0544e-02, -2.0542e-02],\n",
       "          [-5.0013e-02, -1.1827e-03,  6.8789e-01,  3.1406e+00, -1.1507e-03,\n",
       "           -8.6317e-02,  2.0543e-02, -2.0543e-02],\n",
       "          [-5.1755e-02, -6.5780e-03,  6.7941e-01,  3.1395e+00,  3.8265e-03,\n",
       "           -8.6778e-02,  2.0539e-02, -2.0547e-02]])},\n",
       " 'task_descriptions': ['put the white mug on the plate and put the chocolate pudding to the right of the plate',\n",
       "  'pick up the book and place it in the back compartment of the caddy',\n",
       "  'turn on the stove and put the moka pot on it',\n",
       "  'put the black bowl in the bottom drawer of the cabinet and close it',\n",
       "  'put both the alphabet soup and the tomato sauce in the basket',\n",
       "  'put both the alphabet soup and the tomato sauce in the basket',\n",
       "  'put both the alphabet soup and the tomato sauce in the basket',\n",
       "  'put both the cream cheese box and the butter in the basket']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "obs_venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27be11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def center_crop_image(image: Image.Image) -> Image.Image:\n",
    "\n",
    "    crop_scale = 0.9\n",
    "    orig_w, orig_h = image.size\n",
    "    image_tensor = F.to_tensor(image)\n",
    "    crop_h = int(orig_h * crop_scale)\n",
    "    crop_w = int(orig_w * crop_scale)\n",
    "    image_tensor = F.center_crop(image_tensor, (crop_h, crop_w))\n",
    "    image_tensor = F.resize(image_tensor, (orig_h, orig_w))\n",
    "    final_image = F.to_pil_image(image_tensor)\n",
    "    \n",
    "    final_image = final_image.convert(\"RGB\")\n",
    "    return final_image\n",
    "\n",
    "# def process_input(inputs:list, task_descriptions:list, config):\n",
    "    \n",
    "#     batchdata = {\"input_ids\":[],\"attention_mask\":[],\"pixel_values\":[]}  \n",
    "    \n",
    "#     for i in range(len(inputs)):\n",
    "#         input = inputs[i]\n",
    "#         task_description = task_descriptions[i]\n",
    "        \n",
    "#         image = Image.fromarray(input[\"full_image\"]).convert(\"RGB\")\n",
    "#         if config[\"center_crop\"]:\n",
    "#             image = center_crop_image(image)\n",
    "#         prompt = f\"In: What action should the robot take to {task_description.lower()}?\\nOut:\"\n",
    "#         batch_feature  = processor(prompt, image)\n",
    "        \n",
    "#         if \"wrist_image\" in input.keys():\n",
    "#             wrist_image = Image.fromarray(input[\"wrist_image\"]).convert(\"RGB\")\n",
    "#             if config[\"center_crop\"]:\n",
    "#                 wrist_image = center_crop_image(wrist_image)\n",
    "#             wrist_batch_feature = processor(prompt, wrist_image)\n",
    "#             primary_pixel_values = batch_feature[\"pixel_values\"]\n",
    "#             batch_feature[\"pixel_values\"] = torch.cat([primary_pixel_values] + [wrist_batch_feature[\"pixel_values\"]], dim=1)\n",
    "            \n",
    "#         input_ids = batch_feature[\"input_ids\"]\n",
    "#         attention_mask = batch_feature[\"attention_mask\"]\n",
    "#         pixel_values = batch_feature[\"pixel_values\"]\n",
    "        \n",
    "#         if not torch.all(input_ids[:, -1] == 29871):\n",
    "#             input_ids = torch.cat(\n",
    "#                 (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "#             )\n",
    "#             attention_mask = torch.cat(\n",
    "#                 (attention_mask, torch.unsqueeze(torch.Tensor([True]).bool(), dim=0).to(attention_mask.device)), dim=1\n",
    "#             )\n",
    "        \n",
    "#         batchdata[\"input_ids\"].append(input_ids)    \n",
    "#         batchdata[\"attention_mask\"].append(attention_mask)    \n",
    "#         batchdata[\"pixel_values\"].append(pixel_values)    \n",
    "    \n",
    "    \n",
    "#     device = torch.device('cuda') \n",
    "    \n",
    "#     batchdata[\"input_ids\"] = [x.transpose(0, 1) for x in batchdata[\"input_ids\"]]\n",
    "#     batchdata[\"attention_mask\"] = [x.transpose(0, 1) for x in batchdata[\"attention_mask\"]]\n",
    "#     batchdata[\"input_ids\"] = pad_sequence(batchdata[\"input_ids\"], batch_first=True, padding_value=processor.tokenizer.pad_token_id).squeeze(-1).to(device)\n",
    "#     batchdata[\"attention_mask\"] = pad_sequence(batchdata[\"attention_mask\"], batch_first=True, padding_value=0).squeeze(-1).to(device)\n",
    "    \n",
    "#     padding_mask = batchdata[\"input_ids\"].ne(processor.tokenizer.pad_token_id)\n",
    "#     assert  torch.all(padding_mask==batchdata[\"attention_mask\"].ne(0))\n",
    "#     padding_mask = ~padding_mask\n",
    "#     padding_mask = padding_mask.int() \n",
    "#     sorted_indices = torch.argsort(padding_mask, dim=1, descending=True, stable=True)\n",
    "#     batchdata[\"input_ids\"] = torch.gather(batchdata[\"input_ids\"], 1, sorted_indices)\n",
    "#     batchdata[\"attention_mask\"] = torch.gather(batchdata[\"attention_mask\"], 1, sorted_indices)\n",
    "    \n",
    "    \n",
    "#     batchdata[\"pixel_values\"] = torch.cat(batchdata[\"pixel_values\"] , dim=0).to(device)\n",
    "#     assert torch.all(batchdata[\"attention_mask\"].ne(0) == batchdata[\"input_ids\"].ne(processor.tokenizer.pad_token_id))\n",
    "\n",
    "#     return batchdata\n",
    "def process_input(venv_obs, config):\n",
    "    \n",
    "    batchdata = {\"input_ids\":[],\"attention_mask\":[],\"pixel_values\":[]}  \n",
    "    \n",
    "    task_descriptions = venv_obs[\"task_descriptions\"]\n",
    "    images_and_states = venv_obs[\"images_and_states\"]\n",
    "    for i in range(len(venv_obs[\"task_descriptions\"])):\n",
    "        task_description = task_descriptions[i]\n",
    "        \n",
    "        image = Image.fromarray(images_and_states[\"full_image\"][i].numpy()).convert(\"RGB\")\n",
    "        if config[\"center_crop\"]:\n",
    "            image = center_crop_image(image)\n",
    "        prompt = f\"In: What action should the robot take to {task_description.lower()}?\\nOut:\"\n",
    "        batch_feature  = processor(prompt, image)\n",
    "        \n",
    "        # if \"wrist_image\" in venv_obs.keys():\n",
    "        #     wrist_image = Image.fromarray(input[\"wrist_image\"]).convert(\"RGB\")\n",
    "        #     if config[\"center_crop\"]:\n",
    "        #         wrist_image = center_crop_image(wrist_image)\n",
    "        #     wrist_batch_feature = processor(prompt, wrist_image)\n",
    "        #     primary_pixel_values = batch_feature[\"pixel_values\"]\n",
    "        #     batch_feature[\"pixel_values\"] = torch.cat([primary_pixel_values] + [wrist_batch_feature[\"pixel_values\"]], dim=1)\n",
    "            \n",
    "        input_ids = batch_feature[\"input_ids\"]\n",
    "        attention_mask = batch_feature[\"attention_mask\"]\n",
    "        pixel_values = batch_feature[\"pixel_values\"]\n",
    "        \n",
    "        if not torch.all(input_ids[:, -1] == 29871):\n",
    "            input_ids = torch.cat(\n",
    "                (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                (attention_mask, torch.unsqueeze(torch.Tensor([True]).bool(), dim=0).to(attention_mask.device)), dim=1\n",
    "            )\n",
    "        \n",
    "        batchdata[\"input_ids\"].append(input_ids)    \n",
    "        batchdata[\"attention_mask\"].append(attention_mask)    \n",
    "        batchdata[\"pixel_values\"].append(pixel_values)    \n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda') \n",
    "    \n",
    "    batchdata[\"input_ids\"] = [x.transpose(0, 1) for x in batchdata[\"input_ids\"]]\n",
    "    batchdata[\"attention_mask\"] = [x.transpose(0, 1) for x in batchdata[\"attention_mask\"]]\n",
    "    batchdata[\"input_ids\"] = pad_sequence(batchdata[\"input_ids\"], batch_first=True, padding_value=processor.tokenizer.pad_token_id).squeeze(-1).to(device)\n",
    "    batchdata[\"attention_mask\"] = pad_sequence(batchdata[\"attention_mask\"], batch_first=True, padding_value=0).squeeze(-1).to(device)\n",
    "    \n",
    "    padding_mask = batchdata[\"input_ids\"].ne(processor.tokenizer.pad_token_id)\n",
    "    assert  torch.all(padding_mask==batchdata[\"attention_mask\"].ne(0))\n",
    "    padding_mask = ~padding_mask\n",
    "    padding_mask = padding_mask.int() \n",
    "    sorted_indices = torch.argsort(padding_mask, dim=1, descending=True, stable=True)\n",
    "    batchdata[\"input_ids\"] = torch.gather(batchdata[\"input_ids\"], 1, sorted_indices)\n",
    "    batchdata[\"attention_mask\"] = torch.gather(batchdata[\"attention_mask\"], 1, sorted_indices)\n",
    "    \n",
    "    \n",
    "    batchdata[\"pixel_values\"] = torch.cat(batchdata[\"pixel_values\"] , dim=0).to(device)\n",
    "    assert torch.all(batchdata[\"attention_mask\"].ne(0) == batchdata[\"input_ids\"].ne(processor.tokenizer.pad_token_id))\n",
    "\n",
    "    return batchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87aeed",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# DO_SAMPLE = True\n",
    "DO_SAMPLE = False\n",
    "TEMP = 1.6\n",
    "# TEMP = 0.2\n",
    "UNNORM_KEY = \"libero_10\"\n",
    "UNNORM_KEY = f\"{UNNORM_KEY}_no_noops\"\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "def pad_sequence_to_length(tensors, max_seq_len, pad_token_id, left_pad=False):\n",
    "    \"\"\"\n",
    "    pad a 2D tensors (e.g. responses, logprobs) in the last dim to max_seq_length.\n",
    "    input shape: [bs, seq_length]\n",
    "    output shape: [bs, max_seq_length]\n",
    "    (0, max_seq_len - tensors.shape[-1]) means right pad to max_seq_length and no left pad\n",
    "    \"\"\"\n",
    "    if tensors.shape[-1] >= max_seq_len:\n",
    "        return tensors\n",
    "    pad_tuple = (max_seq_len - tensors.shape[-1], 0) if left_pad else (0, max_seq_len - tensors.shape[-1])\n",
    "    return torch.nn.functional.pad(tensors, pad_tuple, 'constant', pad_token_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _generate_one_step(prompts: dict):\n",
    "    idx = prompts['input_ids']  # (bs, prompt_length)\n",
    "    attention_mask = prompts['attention_mask']  # left-padded attention_mask\n",
    "    pixel_values = prompts[\"pixel_values\"]\n",
    "\n",
    "\n",
    "\n",
    "    # make sampling args can be overriden by inputs\n",
    "    do_sample = prompts.get('do_sample', DO_SAMPLE)\n",
    "\n",
    "\n",
    "    temperature = prompts.get('temperature', TEMP)\n",
    "\n",
    "    #generation_config = GenerationConfig(temperature=temperature, top_p=top_p, top_k=top_k)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        actions, response = actor_module.generate_action_verl(\n",
    "            input_ids=idx,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            padding_idx = processor.tokenizer.pad_token_id,\n",
    "            do_sample=do_sample,\n",
    "            unnorm_key=UNNORM_KEY,\n",
    "            temperature=temperature,)\n",
    "    \n",
    "    \n",
    "    assert processor.tokenizer.pad_token_id is not None\n",
    "\n",
    "    assert idx.ndim == 2\n",
    "    idx = pad_sequence_to_length(idx,max_seq_len=MAX_PROMPT_LENGTH,pad_token_id=processor.tokenizer.pad_token_id,left_pad=True)\n",
    "    \n",
    "    assert attention_mask.ndim == 2\n",
    "    attention_mask = pad_sequence_to_length(attention_mask,max_seq_len=MAX_PROMPT_LENGTH,pad_token_id=0,left_pad=True)\n",
    "    \n",
    "    \n",
    "    assert idx.device.type == 'cuda'\n",
    "    assert response.device.type == 'cuda'\n",
    "    #assert seq.device.type == 'cuda'\n",
    "    assert attention_mask.device.type == 'cuda'\n",
    "    assert pixel_values.device.type == 'cuda'\n",
    "    batch ={\n",
    "            'responses': response,\n",
    "            'input_ids': idx,\n",
    "            'attention_mask': attention_mask,\n",
    "            \"pixel_values\":pixel_values,\n",
    "            \"action\":actions,\n",
    "        }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07979179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task description: ['put the white mug on the plate and put the chocolate pudding to the right of the plate', 'pick up the book and place it in the back compartment of the caddy', 'turn on the stove and put the moka pot on it', 'put the black bowl in the bottom drawer of the cabinet and close it', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the cream cheese box and the butter in the basket']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1/64 [00:02<02:42,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 63/64 [01:59<00:01,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get task and trial id [ 50 100 150 200 250 300 350 400 450 500] [406 324 456 251 303 485 364 316] [8, 6, 9, 5, 6, 9, 7, 6] [6, 24, 6, 1, 3, 35, 14, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [02:25<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 71:\n",
      "  Truncated: [[False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]\n",
      " [False False False False False False False  True]]\n",
      "  Info: {'final_observation': {'images_and_states': {'full_image': tensor([[[[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 115, 112],\n",
      "          [117, 115, 111],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [118, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [117, 114, 111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 79,  57,  45],\n",
      "          [ 76,  55,  44],\n",
      "          [ 73,  54,  44],\n",
      "          ...,\n",
      "          [ 83,  57,  45],\n",
      "          [ 84,  58,  46],\n",
      "          [ 85,  59,  47]],\n",
      "\n",
      "         [[ 77,  56,  46],\n",
      "          [ 76,  55,  46],\n",
      "          [ 74,  54,  45],\n",
      "          ...,\n",
      "          [ 82,  57,  44],\n",
      "          [ 82,  58,  45],\n",
      "          [ 83,  58,  46]],\n",
      "\n",
      "         [[ 77,  56,  47],\n",
      "          [ 76,  55,  47],\n",
      "          [ 76,  55,  47],\n",
      "          ...,\n",
      "          [ 80,  57,  44],\n",
      "          [ 81,  57,  44],\n",
      "          [ 81,  58,  45]]],\n",
      "\n",
      "\n",
      "        [[[110, 109, 105],\n",
      "          [111, 109, 106],\n",
      "          [111, 109, 106],\n",
      "          ...,\n",
      "          [110, 108, 105],\n",
      "          [111, 109, 106],\n",
      "          [110, 108, 104]],\n",
      "\n",
      "         [[111, 109, 106],\n",
      "          [111, 109, 106],\n",
      "          [111, 109, 106],\n",
      "          ...,\n",
      "          [111, 109, 106],\n",
      "          [110, 108, 105],\n",
      "          [109, 107, 104]],\n",
      "\n",
      "         [[110, 109, 105],\n",
      "          [110, 109, 105],\n",
      "          [110, 109, 105],\n",
      "          ...,\n",
      "          [102, 101,  99],\n",
      "          [101, 101,  99],\n",
      "          [101, 101,  98]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[139, 108,  67],\n",
      "          [138, 107,  67],\n",
      "          [138, 107,  66],\n",
      "          ...,\n",
      "          [117,  87,  50],\n",
      "          [117,  86,  50],\n",
      "          [117,  86,  50]],\n",
      "\n",
      "         [[139, 107,  66],\n",
      "          [139, 107,  66],\n",
      "          [138, 106,  65],\n",
      "          ...,\n",
      "          [118,  87,  51],\n",
      "          [118,  87,  51],\n",
      "          [117,  87,  50]],\n",
      "\n",
      "         [[138, 106,  65],\n",
      "          [138, 106,  64],\n",
      "          [138, 105,  64],\n",
      "          ...,\n",
      "          [117,  86,  50],\n",
      "          [117,  86,  50],\n",
      "          [117,  86,  50]]],\n",
      "\n",
      "\n",
      "        [[[  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          ...,\n",
      "          [130, 130, 130],\n",
      "          [135, 135, 135],\n",
      "          [140, 140, 140]],\n",
      "\n",
      "         [[  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          ...,\n",
      "          [ 81,  81,  81],\n",
      "          [ 89,  89,  89],\n",
      "          [ 96,  96,  96]],\n",
      "\n",
      "         [[  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          [  0,   0,   0],\n",
      "          ...,\n",
      "          [ 19,  19,  19],\n",
      "          [ 21,  21,  21],\n",
      "          [ 25,  25,  25]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[204, 187, 171],\n",
      "          [199, 182, 166],\n",
      "          [195, 178, 162],\n",
      "          ...,\n",
      "          [200, 183, 162],\n",
      "          [201, 184, 164],\n",
      "          [202, 185, 165]],\n",
      "\n",
      "         [[201, 184, 168],\n",
      "          [196, 180, 164],\n",
      "          [195, 178, 162],\n",
      "          ...,\n",
      "          [199, 182, 162],\n",
      "          [200, 183, 162],\n",
      "          [202, 185, 164]],\n",
      "\n",
      "         [[200, 183, 167],\n",
      "          [194, 177, 161],\n",
      "          [194, 177, 161],\n",
      "          ...,\n",
      "          [199, 181, 161],\n",
      "          [199, 182, 162],\n",
      "          [201, 184, 164]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 115, 112],\n",
      "          [117, 115, 111],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [118, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [117, 114, 111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 79,  57,  45],\n",
      "          [ 76,  55,  44],\n",
      "          [ 73,  54,  44],\n",
      "          ...,\n",
      "          [ 83,  57,  45],\n",
      "          [ 84,  58,  46],\n",
      "          [ 85,  59,  47]],\n",
      "\n",
      "         [[ 77,  56,  46],\n",
      "          [ 76,  55,  46],\n",
      "          [ 74,  54,  45],\n",
      "          ...,\n",
      "          [ 82,  57,  44],\n",
      "          [ 82,  58,  45],\n",
      "          [ 83,  58,  46]],\n",
      "\n",
      "         [[ 77,  56,  47],\n",
      "          [ 76,  55,  47],\n",
      "          [ 76,  55,  47],\n",
      "          ...,\n",
      "          [ 80,  57,  44],\n",
      "          [ 81,  57,  44],\n",
      "          [ 81,  58,  45]]],\n",
      "\n",
      "\n",
      "        [[[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 115, 112],\n",
      "          [117, 115, 111],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [118, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [117, 114, 111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 79,  57,  45],\n",
      "          [ 76,  55,  44],\n",
      "          [ 73,  54,  44],\n",
      "          ...,\n",
      "          [ 83,  57,  45],\n",
      "          [ 84,  58,  46],\n",
      "          [ 85,  59,  47]],\n",
      "\n",
      "         [[ 77,  56,  46],\n",
      "          [ 76,  55,  46],\n",
      "          [ 74,  54,  45],\n",
      "          ...,\n",
      "          [ 82,  57,  44],\n",
      "          [ 82,  58,  45],\n",
      "          [ 83,  58,  46]],\n",
      "\n",
      "         [[ 77,  56,  47],\n",
      "          [ 76,  55,  47],\n",
      "          [ 76,  55,  47],\n",
      "          ...,\n",
      "          [ 80,  57,  44],\n",
      "          [ 81,  57,  44],\n",
      "          [ 81,  58,  45]]],\n",
      "\n",
      "\n",
      "        [[[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 115, 112],\n",
      "          [117, 115, 111],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [117, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [116, 114, 110]],\n",
      "\n",
      "         [[ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          [ 13,  13,  13],\n",
      "          ...,\n",
      "          [118, 116, 112],\n",
      "          [116, 114, 110],\n",
      "          [117, 114, 111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 79,  57,  45],\n",
      "          [ 76,  55,  44],\n",
      "          [ 73,  54,  44],\n",
      "          ...,\n",
      "          [ 83,  57,  45],\n",
      "          [ 84,  58,  46],\n",
      "          [ 85,  59,  47]],\n",
      "\n",
      "         [[ 77,  56,  46],\n",
      "          [ 76,  55,  46],\n",
      "          [ 74,  54,  45],\n",
      "          ...,\n",
      "          [ 82,  57,  44],\n",
      "          [ 82,  58,  45],\n",
      "          [ 83,  58,  46]],\n",
      "\n",
      "         [[ 77,  56,  47],\n",
      "          [ 76,  55,  47],\n",
      "          [ 76,  55,  47],\n",
      "          ...,\n",
      "          [ 80,  57,  44],\n",
      "          [ 81,  57,  44],\n",
      "          [ 81,  58,  45]]]], dtype=torch.uint8), 'state': tensor([[ 0.0513, -0.0419,  0.4633,  2.4989,  1.1025, -0.1962,  0.0397, -0.0400],\n",
      "        [-0.2333,  0.1297,  1.0269,  2.5995, -1.8694, -0.1193,  0.0307, -0.0250],\n",
      "        [ 0.0621, -0.0743,  1.0094,  0.6793,  3.1835,  0.5987,  0.0400, -0.0400],\n",
      "        [ 0.0195, -0.0495,  1.0053,  2.1659, -2.0481, -0.1144,  0.0389, -0.0393],\n",
      "        [ 0.0424, -0.1390,  0.5202,  2.8142,  1.0873, -0.1187,  0.0399, -0.0400],\n",
      "        [-0.0038, -0.1155,  0.4505,  2.6675,  1.2084, -0.3891,  0.0399, -0.0402],\n",
      "        [ 0.0124, -0.1451,  0.4519,  3.0655, -0.1561, -0.1262,  0.0399, -0.0400],\n",
      "        [ 0.1076, -0.1940,  0.4472,  3.1205, -0.1352, -0.0507,  0.0400, -0.0394]])}, 'task_descriptions': ['put the white mug on the plate and put the chocolate pudding to the right of the plate', 'pick up the book and place it in the back compartment of the caddy', 'turn on the stove and put the moka pot on it', 'put the black bowl in the bottom drawer of the cabinet and close it', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the alphabet soup and the tomato sauce in the basket', 'put both the cream cheese box and the butter in the basket']}, 'final_info': {'env_id': [0, 1, 2, 3, 4, 5, 6, 7], 'episode': {'success_once': tensor([False, False, False, False, False, False, False, False]), 'return': tensor([0., 0., 0., 0., 0., 0., 0., 0.]), 'episode_len': tensor([512, 512, 512, 512, 512, 512, 512, 512], dtype=torch.int32), 'reward': tensor([0., 0., 0., 0., 0., 0., 0., 0.])}}, '_final_info': array([ True,  True,  True,  True,  True,  True,  True,  True]), '_final_observation': array([ True,  True,  True,  True,  True,  True,  True,  True]), '_elapsed_steps': array([ True,  True,  True,  True,  True,  True,  True,  True])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "task_descriptions = obs_venv['task_descriptions']\n",
    "print(f\"Task description: {task_descriptions}\")\n",
    "config = {\n",
    "    \"center_crop\": True,\n",
    "    \"num_steps_wait\": 10\n",
    "}\n",
    "vla_history = []\n",
    "for step in trange(max_steps // action_chunks_len):\n",
    "\n",
    "    # print(\"Step:\", step)\n",
    "    # prof.step()\n",
    "    vla_input = process_input(obs_venv, config)\n",
    "    # vla_input.update(meta_info)\n",
    "    vla_output = _generate_one_step(vla_input)\n",
    "    actions = vla_output[\"action\"]\n",
    "    if step < 1:\n",
    "        actions = torch.tensor([[0, 0, 0, 0, 0, 0, 1]]).repeat(batch_size, action_chunks_len, 1).numpy()\n",
    "        print(actions.shape)    \n",
    "    step_data = {\n",
    "        \"responses\": vla_output[\"responses\"],\n",
    "        \"input_ids\": vla_output[\"input_ids\"],\n",
    "        \"attention_mask\": vla_output[\"attention_mask\"],\n",
    "        \"pixel_values\": vla_output[\"pixel_values\"],\n",
    "        \"action\": actions,\n",
    "        \"step\": step\n",
    "    }\n",
    "    vla_history.append(step_data)\n",
    "\n",
    "    # TODO(caiyunke.astra): check if need to invert the gripper action\n",
    "    # Invert the gripper action\n",
    "    actions[..., -1] =actions[..., -1] *  -1.0\n",
    "    \n",
    "\n",
    "    # result = libero_env.step(actions[0])\n",
    "    result = libero_env.chunk_step(actions)\n",
    "    obs_venv, _, terminated_venv, truncated_venv, info_venv = result\n",
    "    step += action_chunks_len\n",
    "    if terminated_venv.any() or truncated_venv.any():\n",
    "        print(f\"Step {step}:\")\n",
    "        if terminated_venv.any():\n",
    "            print(f\"  Terminated: {terminated_venv.cpu().numpy()}\")\n",
    "        if truncated_venv.any():\n",
    "            print(f\"  Truncated: {truncated_venv.cpu().numpy()}\")\n",
    "        if info_venv:\n",
    "            print(f\"  Info: {info_venv}\")\n",
    "    # if complete:\n",
    "    #     print(f\"Task completed at step {step}.\")\n",
    "    #     break\n",
    "success_count = info_venv[\"final_info\"][\"episode\"][\"success_once\"].int().sum().item()\n",
    "print(f\"  success rate: {success_count} / {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "libero_env.flush_video(video_sub_dir=f\"episode_{7}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f58cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "libero_env.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
