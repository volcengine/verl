# the rob_ppo config will override default ppo_trainer.yaml

hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

env:
  rollout:
    pipeline_stage_num: 2
  actor:
    model:
      num_action_chunks: 8
      action_dim: 7
  train:
    simulator_type: libero
    max_episode_steps: 512
    reward_coef: 1.0
    only_eval: False
    video_cfg:
      save_video: True
      video_base_dir: /tmp/videos
    num_envs: 16
    seed: 42
    task_suite_name: libero_10

    # ============================================================
    # Isaac Simulation Mode Configuration
    # ============================================================
    # Two modes available:
    # 1. Standard mode (isaac_server_mode=false):
    #    - Each EnvWorker runs local Isaac instances
    #    - Simple but limited scalability
    #
    # 2. Isaac Server mode (isaac_server_mode=true):
    #    - Isaac Sim runs as Ray-managed IsaacServers
    #    - Ray manages all resources uniformly
    #    - Better scalability and resource management
    # ============================================================
    
    # Enable Isaac server mode (recommended for production)
    isaac_server_mode: False
    
    # Number of EnvWorker instances per node in Isaac server mode
    # These are lightweight clients, so typically only need 1
    server_mode_env_workers: 1
    
    # Number of Isaac servers per stage (only for isaac_server_mode=true)
    # Typically equals the number of simulation GPUs
    num_isaac_servers: 8
    
    # Number of tasks in the multi-task environment
    num_tasks: 10
    
    # Envs per task (only for Ray actor mode)
    # sim_envs_per_stage = num_tasks × group_size
    group_size: 16
    
    # Original env worker count (how many workers would exist in standard mode)
    # This tells the coordinator how many stages to handle
    # total_stages = original_env_worker_count × pipeline_stage_num
    original_env_worker_count: 8
    
    # Total trajectories for training (for EnvWorkerServer coordination)
    # This is the number of trajectories participating in training, NOT sim envs
    total_trajs: 128
    
    # Environment ID for Isaac Lab
    env_id: "Isaac-Libero-Franka-OscPose-Camera-All-Tasks-v0"

    init_params:
      camera_depths: False
      camera_heights: 256
      camera_widths: 256
      camera_names: 
        - agentview
        - robot0_eye_in_hand
    
    # Profile the env worker
    profiler:

      # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
      _target_: verl.utils.profiler.ProfilerConfig

      # Profiling tool to use
      # options: nsys, npu, torch, torch_memory
      # Defaults to global_profiler.tool if set
      tool: ${oc.select:global_profiler.tool,null}

      # Whether to enable profiling for env worker
      enable: False

      # Whether to profile all ranks
      all_ranks: False

      # List of ranks to profile (empty means no specific ranks)
      ranks: []

      # Path to save profiling results
      # Defaults to global_profiler.save_path if set
      save_path: ${oc.select:global_profiler.save_path,null}

      # Tool-specific configurations
      tool_config:

        # nsys tool config
        nsys:

          # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
          _target_: verl.utils.profiler.config.NsightToolConfig
        
          # True for each task has its own database, False for all tasks in one training step share one database.
          discrete: ${oc.select:global_profiler.global_tool_config.nsys.discrete}
        
        # npu config
        npu:

          # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
          _target_: verl.utils.profiler.config.NPUToolConfig

          # Contents to profile, can be empty
          # options: npu, cpu, memory, shapes, module, stack
          contents: []

          # Collection level, optional values: level_none, level0, level1, level2.
          level: "level1"

          # Whether to automatically parse the data.
          analysis: True

          # True for each task has its own database, False for all tasks in one training step share one database.
          discrete: False
        
        # torch profiler config
        torch:

          # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
          _target_: verl.utils.profiler.config.TorchProfilerToolConfig

          # Contents to profile, can be empty
          # options: cuda, cpu, memory, shapes, stack
          contents: []

          # True for each task has its own database, False for all tasks in one training step share one database.
          discrete: False


        # torch memory profiler config
        torch_memory:

          # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs
          _target_: verl.utils.profiler.config.TorchMemoryToolConfig

          # Maximum number of memory allocation entries to track
          trace_alloc_max_entries: ${oc.select:global_profiler.global_tool_config.torch_memory.trace_alloc_max_entries,100000}

          # Stack trace depth for memory allocations
          stack_depth: ${oc.select:global_profiler.global_tool_config.torch_memory.stack_depth,32}
  disagg_sim:
    enable: False
    nnodes: 1


actor_rollout_ref:
  actor:
    num_images_in_input: 1
    traj_mini_batch_size: 16
    fsdp_config:
      wrap_policy:
        transformer_layer_cls_to_wrap: 
          - PrismaticProjector
          - LlamaDecoderLayer
        min_num_params: 0
      param_offload: False
      optimizer_offload: False
      forward_prefetch: True
      fsdp_size: -1
  rollout:
    mode: async_envloop
    prompt_length: 512
